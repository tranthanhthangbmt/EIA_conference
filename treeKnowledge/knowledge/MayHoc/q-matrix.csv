question_id,content,options,answer,skill_id_list,difficulty
Chg1_Q1_MCQ,"Học máy (Machine Learning) là gì?","[""A. Khoa học (và nghệ thuật) lập trình máy tính để chúng có thể học từ dữ liệu."", ""B. Việc tạo ra các robot có hình dáng giống con người."", ""C. Lập trình máy tính để thực hiện các tác vụ cụ thể mà không cần dữ liệu."", ""D. Một loại phần cứng máy tính mới.""]",A,"[""1.1_KhaiNiem_ML""]",easy
Chg1_Q2_MCQ,"Tại sao Machine Learning ngày càng trở nên phổ biến hiện nay?","[""A. Vì ổ cứng ngày càng rẻ."", ""B. Vì dữ liệu lớn (Big Data) và năng lực tính toán tăng mạnh."", ""C. Vì lập trình truyền thống đã lỗi thời."", ""D. Vì nó không cần con người can thiệp.""]",B,"[""1.2_TaiSao_ML""]",easy
Chg1_Q3_MCQ,"Trong 'Học có giám sát' (Supervised Learning), dữ liệu huấn luyện bao gồm những gì?","[""A. Chỉ các đặc trưng (features) đầu vào, không có nhãn."", ""B. Các đặc trưng (features) và nhãn (labels) mong muốn."", ""C. Chỉ các nhãn (labels), không có đặc trưng."", ""D. Dữ liệu ngẫu nhiên không có cấu trúc.""]",B,"[""1.3_CacLoaiHeThong""]",easy
Chg1_Q4_MCQ,"Thuật toán phân cụm (Clustering) thuộc loại hệ thống học máy nào?","[""A. Học có giám sát (Supervised Learning)."", ""B. Học không giám sát (Unsupervised Learning)."", ""C. Học tăng cường (Reinforcement Learning)."", ""D. Học bán giám sát (Semi-supervised Learning).""]",B,"[""1.3_CacLoaiHeThong""]",medium
Chg1_Q5_MCQ,"Vấn đề 'Overfitting' (Quá khớp) trong mô hình học máy là gì?","[""A. Mô hình quá đơn giản để nắm bắt cấu trúc dữ liệu."", ""B. Mô hình hoạt động tốt trên tập kiểm tra nhưng kém trên tập huấn luyện."", ""C. Mô hình hoạt động rất tốt trên dữ liệu huấn luyện nhưng không tổng quát hóa tốt trên dữ liệu mới."", ""D. Mô hình có quá ít tham số.""]",C,"[""1.4_ThachThuc_Chinh""]",medium
Chg1_Q6_MCQ,"Điều gì xảy ra khi dữ liệu huấn luyện quá ít hoặc không đại diện cho tổng thể (Sampling Bias)?","[""A. Mô hình sẽ học nhanh hơn."", ""B. Mô hình dự đoán sai lệch và thiếu chính xác trên thực tế."", ""C. Không ảnh hưởng gì nếu dùng thuật toán tốt."", ""D. Mô hình sẽ tự động sinh ra dữ liệu mới.""]",B,"[""1.4_ThachThuc_Chinh""]",medium
Chg1_Q7_MCQ,"Tập dữ liệu kiểm tra (Test set) được sử dụng để làm gì?","[""A. Để huấn luyện mô hình."", ""B. Để tinh chỉnh các siêu tham số (hyperparameters)."", ""C. Để ước tính sai số tổng quát hóa (generalization error) của mô hình cuối cùng."", ""D. Để loại bỏ nhiễu khỏi dữ liệu.""]",C,"[""1.5_KiemThu_DanhGia""]",medium
Chg1_Q8_MCQ,"Mục đích chính của tập xác thực (Validation set) là gì?","[""A. Để đánh giá hiệu suất cuối cùng của mô hình."", ""B. Để so sánh các mô hình khác nhau và tinh chỉnh siêu tham số."", ""C. Để thay thế tập huấn luyện."", ""D. Để lưu trữ dữ liệu dự phòng.""]",B,"[""1.5_KiemThu_DanhGia""]",hard

Chg1_Q9_MCQ,"Học tăng cường (Reinforcement Learning) hoạt động dựa trên cơ chế nào?","[""A. Học từ dữ liệu được gán nhãn chi tiết bởi con người."", ""B. Một tác tử (agent) quan sát môi trường, hành động và nhận thưởng (reward) hoặc phạt (penalty)."", ""C. Phân nhóm dữ liệu dựa trên sự tương đồng mà không cần nhãn."", ""D. Dự đoán giá trị liên tục dựa trên các đặc trưng đầu vào.""]",B,"[""1.3_CacLoaiHeThong""]",medium
Chg1_Q10_MCQ,"Sự khác biệt chính giữa 'Học theo lô' (Batch Learning) và 'Học trực tuyến' (Online Learning) là gì?","[""A. Batch Learning có thể học ngay lập tức khi có dữ liệu mới, còn Online Learning thì không."", ""B. Batch Learning huấn luyện trên toàn bộ dữ liệu có sẵn cùng lúc, trong khi Online Learning huấn luyện dần dần từng mẫu nhỏ."", ""C. Batch Learning dành cho mạng nơ-ron, Online Learning dành cho hồi quy tuyến tính."", ""D. Online Learning không cần dữ liệu.""]",B,"[""1.3_CacLoaiHeThong""]",medium
Chg1_Q11_MCQ,"Học bán giám sát (Semi-supervised Learning) thường được áp dụng khi nào?","[""A. Khi có rất nhiều dữ liệu được gán nhãn."", ""B. Khi hoàn toàn không có dữ liệu nào được gán nhãn."", ""C. Khi có nhiều dữ liệu không nhãn nhưng chỉ có một ít dữ liệu được gán nhãn."", ""D. Khi cần huấn luyện robot chơi game.""]",C,"[""1.3_CacLoaiHeThong""]",medium
Chg1_Q12_MCQ,"Hệ thống lọc thư rác (Spam Filter) là ví dụ điển hình của loại học máy nào?","[""A. Học có giám sát (Supervised Learning - Phân loại)."", ""B. Học có giám sát (Supervised Learning - Hồi quy)."", ""C. Học không giám sát (Unsupervised Learning)."", ""D. Học tăng cường (Reinforcement Learning).""]",A,"[""1.3_CacLoaiHeThong""]",easy
Chg1_Q13_MCQ,"Học dựa trên trường hợp (Instance-based learning) dự đoán dữ liệu mới như thế nào?","[""A. Bằng cách tìm ra một công thức toán học tổng quát cho toàn bộ dữ liệu."", ""B. Bằng cách ghi nhớ các mẫu huấn luyện và so sánh mức độ tương đồng của dữ liệu mới với chúng."", ""C. Bằng cách xây dựng cây quyết định."", ""D. Bằng cách tối ưu hóa hàm mất mát.""]",B,"[""1.3_CacLoaiHeThong""]",hard
Chg1_Q14_MCQ,"Vấn đề 'Underfitting' (Chưa khớp/Thiếu khớp) xảy ra khi nào?","[""A. Khi mô hình quá phức tạp so với dữ liệu."", ""B. Khi mô hình quá đơn giản để nắm bắt cấu trúc cơ bản của dữ liệu."", ""C. Khi dữ liệu có quá nhiều nhiễu."", ""D. Khi tập kiểm tra quá nhỏ.""]",B,"[""1.4_ThachThuc_Chinh""]",medium
Chg1_Q15_MCQ,"Cách nào sau đây KHÔNG giúp giảm thiểu vấn đề Overfitting?","[""A. Thu thập thêm dữ liệu huấn luyện."", ""B. Giảm bớt độ phức tạp của mô hình (ví dụ: giảm bậc đa thức)."", ""C. Sử dụng các kỹ thuật Regularization (Điều chuẩn)."", ""D. Làm cho mô hình phức tạp hơn với nhiều tham số hơn.""]",D,"[""1.4_ThachThuc_Chinh""]",medium
Chg1_Q16_MCQ,"Nếu mô hình của bạn hoạt động kém trên cả tập huấn luyện và tập kiểm tra, khả năng cao nó đang gặp vấn đề gì?","[""A. Overfitting."", ""B. Underfitting."", ""C. Data Mismatch."", ""D. High Variance.""]",B,"[""1.4_ThachThuc_Chinh""]",medium
Chg1_Q17_MCQ,"Kỹ thuật Feature Engineering (Kỹ thuật đặc trưng) bao gồm các hoạt động nào?","[""A. Chỉ thu thập dữ liệu thô."", ""B. Lựa chọn đặc trưng, trích xuất đặc trưng và tạo ra đặc trưng mới hữu ích hơn."", ""C. Xóa bỏ toàn bộ dữ liệu thiếu."", ""D. Tăng kích thước ổ cứng lưu trữ.""]",B,"[""1.4_ThachThuc_Chinh""]",hard
Chg1_Q18_MCQ,"Định lý 'No Free Lunch' (Không có bữa trưa miễn phí) trong Machine Learning ngụ ý điều gì?","[""A. Mọi phần mềm ML đều phải trả phí."", ""B. Không có mô hình nào là tốt nhất cho mọi bài toán; ta phải thử nghiệm và đánh giá."", ""C. Dữ liệu luôn tốn kém để thu thập."", ""D. Luôn luôn có một thuật toán giải quyết được mọi vấn đề.""]",B,"[""1.5_KiemThu_DanhGia""]",hard

Chg1_Q19_MCQ,"Mục tiêu chính của thuật toán 'Giảm chiều dữ liệu' (Dimensionality Reduction) là gì?","[""A. Để tăng số lượng đặc trưng nhằm làm mô hình phức tạp hơn."", ""B. Để nén dữ liệu, giảm dung lượng lưu trữ và giúp mô hình chạy nhanh hơn mà vẫn giữ được thông tin quan trọng."", ""C. Để biến đổi dữ liệu số thành dữ liệu chữ."", ""D. Để tạo ra nhiều dữ liệu giả lập hơn.""]",B,"[""1.3_CacLoaiHeThong""]",medium
Chg1_Q20_MCQ,"Phát hiện bất thường (Anomaly Detection) thường được ứng dụng trong trường hợp nào?","[""A. Dự đoán giá nhà."", ""B. Phân loại ảnh chó mèo."", ""C. Phát hiện gian lận thẻ tín dụng hoặc lỗi sản phẩm trong dây chuyền sản xuất."", ""D. Gợi ý phim trên Netflix.""]",C,"[""1.3_CacLoaiHeThong""]",easy
Chg1_Q21_MCQ,"Trong Học trực tuyến (Online Learning), tham số 'Tốc độ học' (Learning Rate) quyết định điều gì?","[""A. Tốc độ tải dữ liệu từ Internet."", ""B. Mức độ nhanh chóng mà mô hình thích nghi với dữ liệu mới và quên đi dữ liệu cũ."", ""C. Số lượng server cần thiết để chạy mô hình."", ""D. Độ chính xác tối đa của mô hình.""]",B,"[""1.3_CacLoaiHeThong""]",hard
Chg1_Q22_MCQ,"Học kết hợp (Association Rule Learning) thường được sử dụng để làm gì?","[""A. Phân tích giỏ hàng (Market Basket Analysis) để tìm ra các sản phẩm hay được mua cùng nhau."", ""B. Phân loại khách hàng thành các nhóm."", ""C. Dự báo doanh thu tháng sau."", ""D. Nhận diện khuôn mặt.""]",A,"[""1.3_CacLoaiHeThong""]",medium
Chg1_Q23_MCQ,"Kỹ thuật 'Out-of-core learning' giải quyết vấn đề gì?","[""A. Khi CPU quá nóng."", ""B. Khi dữ liệu quá lớn không thể chứa hết trong bộ nhớ RAM (RAM), cần chia nhỏ để huấn luyện dần."", ""C. Khi không có kết nối Internet."", ""D. Khi ổ cứng bị đầy.""]",B,"[""1.3_CacLoaiHeThong""]",hard
Chg1_Q24_MCQ,"Sự khác biệt giữa 'Siêu tham số' (Hyperparameter) và 'Tham số mô hình' (Model Parameter) là gì?","[""A. Không có sự khác biệt."", ""B. Tham số mô hình do người dùng cài đặt trước, siêu tham số do máy tự học."", ""C. Siêu tham số là cấu hình của thuật toán (đặt trước khi huấn luyện), còn tham số mô hình được sinh ra từ dữ liệu trong quá trình huấn luyện."", ""D. Siêu tham số luôn là số nguyên, tham số mô hình là số thực.""]",C,"[""1.5_KiemThu_DanhGia""]",hard
Chg1_Q25_MCQ,"Trong mô hình dựa trên mô hình (Model-based learning), 'Hàm chi phí' (Cost Function) dùng để làm gì?","[""A. Tính toán số tiền điện tiêu thụ."", ""B. Đo lường khoảng cách giữa dự đoán của mô hình và thực tế (sai số), mục tiêu là tối thiểu hóa hàm này."", ""C. Đo lường độ hữu dụng của mô hình."", ""D. Đếm số lượng dữ liệu đầu vào.""]",B,"[""1.3_CacLoaiHeThong""]",medium
Chg1_Q26_MCQ,"Bộ dữ liệu 'Train-Dev Set' (khác với Validation Set) được sử dụng để phát hiện vấn đề gì?","[""A. Overfitting trên tập huấn luyện."", ""B. Data Mismatch (Sự sai lệch dữ liệu) giữa tập huấn luyện và dữ liệu thực tế/production."", ""C. Lỗi phần mềm."", ""D. Tốc độ huấn luyện chậm.""]",B,"[""1.5_KiemThu_DanhGia""]",hard
Chg1_Q27_MCQ,"Nếu dữ liệu chứa nhiều đặc trưng không liên quan (Irrelevant Features), giải pháp tốt nhất là gì?","[""A. Thu thập thêm nhiều dữ liệu nữa."", ""B. Sử dụng thuật toán phức tạp hơn."", ""C. Thực hiện lựa chọn đặc trưng (Feature Selection) để loại bỏ nhiễu."", ""D. Tăng Learning Rate.""]",C,"[""1.4_ThachThuc_Chinh""]",medium
Chg1_Q28_MCQ,"Quy trình 'Cross-Validation' (Kiểm chứng chéo) giúp ích gì so với việc chỉ chia 1 tập Train/Test duy nhất?","[""A. Tiết kiệm thời gian tính toán."", ""B. Tận dụng tối đa dữ liệu để đánh giá mô hình tin cậy hơn, tránh việc kết quả phụ thuộc vào cách chia dữ liệu ngẫu nhiên."", ""C. Làm cho mô hình chạy nhanh hơn."", ""D. Không cần tập kiểm tra (Test set) nữa.""]",B,"[""1.5_KiemThu_DanhGia""]",hard

Chg1_Q29_MCQ,"Tình huống: Bạn xây dựng một mô hình dự đoán giá nhà cho cả nước, nhưng dữ liệu huấn luyện chỉ thu thập từ các thành phố lớn. Khi áp dụng cho vùng nông thôn, mô hình dự đoán sai lệch lớn. Đây là ví dụ của vấn đề gì?","[""A. Overfitting (Quá khớp)."", ""B. Sampling Bias (Thiên kiến lấy mẫu) / Dữ liệu không đại diện."", ""C. Underfitting (Thiếu khớp)."", ""D. Lỗi phần cứng.""]",B,"[""1.4_ThachThuc_Chinh""]",medium
Chg1_Q30_MCQ,"Tình huống: Biểu đồ Learning Curve cho thấy sai số trên tập huấn luyện (Training Error) cao và sai số trên tập kiểm tra (Validation Error) cũng cao tương đương. Bạn nên làm gì?","[""A. Thu thập thêm dữ liệu."", ""B. Sử dụng mô hình phức tạp hơn hoặc tìm thêm các đặc trưng tốt hơn (vì mô hình đang bị Underfitting)."", ""C. Giảm độ phức tạp của mô hình."", ""D. Dừng huấn luyện ngay lập tức.""]",B,"[""1.4_ThachThuc_Chinh""]",hard
Chg1_Q31_MCQ,"Tình huống: Bạn đang huấn luyện mô hình phân loại ảnh. Độ chính xác trên tập Train là 99%, nhưng trên tập Test chỉ đạt 60%. Giải pháp nào sau đây là hợp lý nhất?","[""A. Tiếp tục huấn luyện lâu hơn nữa."", ""B. Áp dụng Regularization (Điều chuẩn), giảm bớt số lượng đặc trưng hoặc thu thập thêm dữ liệu đa dạng hơn."", ""C. Sử dụng mô hình đơn giản hơn nữa."", ""D. Kiểm tra lại nhãn của dữ liệu Test.""]",B,"[""1.5_KiemThu_DanhGia""]",medium
Chg1_Q32_MCQ,"Tình huống: Bạn muốn nhóm các bài báo trên trang tin tức thành các chủ đề tự động (Thể thao, Chính trị,...) nhưng không có nhãn dữ liệu từ trước. Bạn sẽ chọn thuật toán nào?","[""A. Hồi quy tuyến tính (Linear Regression)."", ""B. Phân cụm (Clustering) - Ví dụ: K-Means."", ""C. Cây quyết định (Decision Tree)."", ""D. Học tăng cường (Reinforcement Learning).""]",B,"[""1.3_CacLoaiHeThong""]",medium
Chg1_Q33_MCQ,"Tình huống: Hệ thống dự báo chứng khoán của bạn cần cập nhật liên tục theo từng giây khi có giao dịch mới phát sinh. Chiến lược học nào phù hợp nhất?","[""A. Batch Learning (Học theo lô) - Chạy lại mỗi đêm."", ""B. Online Learning (Học trực tuyến)."", ""C. Offline Learning."", ""D. Transfer Learning.""]",B,"[""1.3_CacLoaiHeThong""]",easy
Chg1_Q34_MCQ,"Tình huống: Bạn có dữ liệu về cảm biến máy móc (nhiệt độ, độ rung...). Đa số dữ liệu là bình thường, thi thoảng mới có sự cố. Bạn muốn máy tự phát hiện khi có sự cố lạ. Đây là bài toán gì?","[""A. Supervised Learning (Phân loại)."", ""B. Anomaly Detection (Phát hiện bất thường) - Unsupervised."", ""C. Regression (Hồi quy)."", ""D. Clustering (Phân cụm).""]",B,"[""1.3_CacLoaiHeThong""]",hard
Chg1_Q35_MCQ,"Tình huống: Bạn huấn luyện mô hình nhận diện giọng nói. Dữ liệu Train lấy từ giọng đọc trong phòng thu (rất sạch), nhưng ứng dụng thực tế dùng trên điện thoại ngoài đường (nhiều tạp âm). Mô hình hoạt động rất tệ. Vấn đề nằm ở đâu?","[""A. Do thuật toán sai."", ""B. Data Mismatch (Sự sai lệch giữa dữ liệu huấn luyện và dữ liệu thực tế)."", ""C. Do Overfitting."", ""D. Do Underfitting.""]",B,"[""1.5_KiemThu_DanhGia""]",medium
Chg1_Q36_MCQ,"Ứng dụng thực tế: Để xây dựng hệ thống gợi ý sản phẩm (Recommender System) như Amazon hay Netflix, người ta thường KHÔNG sử dụng phương pháp nào sau đây?","[""A. Học có giám sát (Supervised Learning) để dự đoán rating."", ""B. Học không giám sát (Unsupervised) để tìm nhóm khách hàng tương đồng."", ""C. Hồi quy tuyến tính đơn biến để dự đoán tất cả."", ""D. Matrix Factorization (Phân rã ma trận).""]",C,"[""1.2_TaiSao_ML""]",hard
Chg1_Q37_MCQ,"Khi nào bạn nên sử dụng 'Học theo lô' (Batch Learning) thay vì 'Học trực tuyến'?","[""A. Khi dữ liệu quá lớn không thể chứa trong ổ cứng."", ""B. Khi tài nguyên tính toán hạn chế và cần mô hình nhẹ."", ""C. Khi dữ liệu ổn định, ít thay đổi theo thời gian và bạn có đủ tài nguyên để huấn luyện lại toàn bộ định kỳ."", ""D. Khi cần mô hình thích nghi ngay lập tức.""]",C,"[""1.3_CacLoaiHeThong""]",medium

Chg1_Pra_Q1,"Để đọc tập dữ liệu từ file CSV (ví dụ 'lifesat.csv') vào một DataFrame bằng thư viện Pandas, câu lệnh nào là chính xác?","[""A. data = pd.read_excel('lifesat.csv')"", ""B. data = pd.read_csv('lifesat.csv')"", ""C. data = pd.load('lifesat.csv')"", ""D. data = open('lifesat.csv') ""]",B,"[""1.6_ThucHanh_TongQuan""]",easy
Chg1_Pra_Q2,"Trong Scikit-Learn, quy trình chuẩn để huấn luyện một mô hình là gọi phương thức nào?","[""A. model.train(X, y)"", ""B. model.learn(X, y)"", ""C. model.fit(X, y)"", ""D. model.compute(X, y)""]",C,"[""1.6_ThucHanh_TongQuan""]",easy
Chg1_Pra_Q3,"Sau khi mô hình đã được huấn luyện, lệnh nào dùng để đưa ra dự đoán cho dữ liệu mới `X_new`?","[""A. model.predict(X_new)"", ""B. model.forecast(X_new)"", ""C. model.guess(X_new)"", ""D. model.estimate(X_new)""]",A,"[""1.6_ThucHanh_TongQuan""]",easy
Chg1_Pra_Q4,"Trong ví dụ của sách, để chuyển đổi từ mô hình Hồi quy tuyến tính (Linear Regression) sang mô hình K-Nearest Neighbors (k-NN) cho bài toán hồi quy, bạn cần thay đổi dòng khởi tạo mô hình thành gì?","[""A. sklearn.neighbors.KNeighborsClassifier()"", ""B. sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)"", ""C. sklearn.cluster.KMeans()"", ""D. sklearn.linear_model.SGDRegressor()""]",B,"[""1.6_ThucHanh_TongQuan""]",medium
Chg1_Pra_Q5,"Scikit-Learn yêu cầu dữ liệu đặc trưng đầu vào (X) phải có định dạng nào khi đưa vào hàm `.fit()`?","[""A. Mảng 1 chiều (1D array) hoặc List đơn giản."", ""B. Mảng 2 chiều (2D array) hoặc DataFrame, ngay cả khi chỉ có một đặc trưng (cần reshape thành (-1, 1))."", ""C. Chuỗi văn bản."", ""D. Dictionary.""]",B,"[""1.6_ThucHanh_TongQuan""]",hard
Chg1_Pra_Q6,"Để nối (merge) hai bảng dữ liệu (ví dụ: bảng GDP và bảng Life Satisfaction) dựa trên một cột chung (ví dụ: 'Country'), ta dùng hàm nào của Pandas?","[""A. pd.concat()"", ""B. pd.merge()"", ""C. pd.join()"", ""D. pd.combine()""]",B,"[""1.6_ThucHanh_TongQuan""]",medium
Chg1_Pra_Q7,"Dòng lệnh `import matplotlib.pyplot as plt` và `df.plot(kind='scatter', ...)` thường được dùng để làm gì trong bước đầu của dự án?","[""A. Để huấn luyện mô hình."", ""B. Để trực quan hóa dữ liệu (Visualize), giúp nhận diện xu hướng hoặc nhiễu."", ""C. Để tải dữ liệu."", ""D. Để lưu mô hình.""]",B,"[""1.6_ThucHanh_TongQuan""]",easy
Chg1_Pra_Q8,"Trong Scikit-Learn, lớp `LinearRegression` nằm trong module nào?","[""A. sklearn.tree"", ""B. sklearn.ensemble"", ""C. sklearn.linear_model"", ""D. sklearn.metrics""]",C,"[""1.6_ThucHanh_TongQuan""]",medium

Chg2_Q1_MCQ,"Khi tìm kiếm dữ liệu thực tế cho dự án Machine Learning, nguồn nào sau đây là phổ biến và uy tín?","[""A. Chỉ sử dụng dữ liệu tự sinh ngẫu nhiên."", ""B. Các kho lưu trữ như UCI Machine Learning Repository, Kaggle, hoặc Amazon AWS Datasets."", ""C. Facebook cá nhân của bạn bè."", ""D. Dữ liệu được viết tay trên giấy.""]",B,"[""2.1_DuLieuThuc""]",easy
Chg2_Q2_MCQ,"RMSE (Root Mean Square Error) thường được sử dụng để đo lường điều gì trong bài toán Hồi quy?","[""A. Độ chính xác (Accuracy) của phân loại."", ""B. Sai số trung bình bình phương của dự đoán so với thực tế."", ""C. Số lượng dữ liệu bị thiếu."", ""D. Tốc độ huấn luyện của mô hình.""]",B,"[""2.2_BucTranhLon""]",medium
Chg2_Q3_MCQ,"Tại sao việc tách tập dữ liệu thành tập huấn luyện (Train) và tập kiểm tra (Test) lại quan trọng ngay từ đầu?","[""A. Để giảm kích thước file dữ liệu."", ""B. Để tránh 'snooping bias' (thiên kiến nhìn trộm) và đảm bảo đánh giá khách quan."", ""C. Để tăng tốc độ huấn luyện."", ""D. Vì máy tính yêu cầu bắt buộc.""]",B,"[""2.3_LayDuLieu""]",easy
Chg2_Q4_MCQ,"Kỹ thuật 'Stratified Sampling' (Lấy mẫu phân tầng) có mục đích gì?","[""A. Lấy mẫu ngẫu nhiên hoàn toàn."", ""B. Đảm bảo tập kiểm tra đại diện cho tổng thể theo tỷ lệ của các thuộc tính quan trọng (ví dụ: giới tính, thu nhập)."", ""C. Chỉ lấy dữ liệu từ một nhóm cụ thể."", ""D. Tăng số lượng dữ liệu.""]",B,"[""2.3_LayDuLieu""]",hard
Chg2_Q5_MCQ,"Trong giai đoạn khám phá dữ liệu (EDA), ma trận tương quan (Correlation Matrix) giúp ta nhận biết điều gì?","[""A. Các thuộc tính nào có mối quan hệ tuyến tính mạnh với nhãn mục tiêu."", ""B. Dữ liệu nào bị thiếu."", ""C. Mô hình nào sẽ hoạt động tốt nhất."", ""D. Độ chính xác của mô hình cuối cùng.""]",A,"[""2.4_KhamPha_TrucQuan""]",medium
Chg2_Q6_MCQ,"Khi xử lý dữ liệu bị thiếu (Missing values), phương pháp 'Imputation' nghĩa là gì?","[""A. Xóa bỏ toàn bộ dòng chứa dữ liệu thiếu."", ""B. Điền vào các giá trị thiếu bằng trung bình (mean), trung vị (median) hoặc một giá trị cố định."", ""C. Để nguyên dữ liệu thiếu cho mô hình tự xử lý."", ""D. Dừng việc huấn luyện ngay lập tức.""]",B,"[""2.5_ChuanBi_DuLieu""]",easy
Chg2_Q7_MCQ,"Mục đích của việc chuẩn hóa dữ liệu (Feature Scaling) như Min-Max Scaling hay Standardization là gì?","[""A. Làm cho dữ liệu đẹp hơn."", ""B. Giúp các thuật toán tối ưu (như Gradient Descent) hội tụ nhanh hơn và hoạt động tốt hơn."", ""C. Biến đổi bài toán hồi quy thành phân loại."", ""D. Giảm số lượng đặc trưng.""]",B,"[""2.5_ChuanBi_DuLieu""]",medium
Chg2_Q8_MCQ,"Kỹ thuật Cross-Validation (Kiểm chứng chéo) K-Fold hoạt động như thế nào?","[""A. Chia dữ liệu thành K phần, huấn luyện trên K-1 phần và kiểm tra trên 1 phần còn lại, lặp lại K lần."", ""B. Chỉ huấn luyện 1 lần trên toàn bộ dữ liệu."", ""C. Chia dữ liệu thành 2 phần bằng nhau."", ""D. Sử dụng K mô hình khác nhau trên cùng một tập dữ liệu.""]",A,"[""2.6_Chon_HuanLuyen_MoHinh""]",hard
Chg2_Q9_MCQ,"Grid Search và Randomized Search được sử dụng để làm gì?","[""A. Để thu thập thêm dữ liệu từ Internet."", ""B. Để tìm ra tập siêu tham số (hyperparameters) tối ưu cho mô hình."", ""C. Để vẽ đồ thị dữ liệu."", ""D. Để triển khai mô hình lên web.""]",B,"[""2.7_TinhChinh_MoHinh""]",medium
Chg2_Q10_MCQ,"Sau khi mô hình đã được huấn luyện và tinh chỉnh xong, bước cuối cùng trước khi đưa vào sản xuất là gì?","[""A. Xóa sạch dữ liệu để tiết kiệm bộ nhớ."", ""B. Đánh giá lại mô hình lần cuối trên tập kiểm tra (Test set) để ước tính sai số tổng quát."", ""C. Chạy mô hình trên tập huấn luyện một lần nữa."", ""D. Thay đổi thuật toán khác.""]",B,"[""2.7_TinhChinh_MoHinh""]",easy
Chg2_Q11_MCQ,"Tại sao cần phải theo dõi (monitor) mô hình sau khi đã triển khai (deploy)?","[""A. Để xem có ai truy cập không."", ""B. Để phát hiện 'Data Drift' hoặc 'Concept Drift' khi hiệu suất mô hình bị suy giảm theo thời gian."", ""C. Luật pháp bắt buộc."", ""D. Không cần theo dõi nếu mô hình đã tốt.""]",B,"[""2.8_TrienKhai_HeThong""]",medium

Chg2_Q12_MCQ,"Trong bài toán hồi quy, khi dữ liệu có nhiều điểm ngoại lai (outliers), chỉ số đánh giá nào thường được ưu tiên hơn RMSE?","[""A. Mean Absolute Error (MAE) vì nó ít nhạy cảm với ngoại lai hơn."", ""B. Accuracy (Độ chính xác)."", ""C. R-squared."", ""D. RMSE vẫn là tốt nhất.""]",A,"[""2.2_BucTranhLon""]",medium
Chg2_Q13_MCQ,"Biểu đồ phân tán (Scatter Plot) hữu ích nhất cho việc gì trong giai đoạn EDA?","[""A. Kiểm tra phân phối của một thuộc tính."", ""B. Hình dung mối quan hệ tương quan giữa hai biến số (ví dụ: thu nhập và giá nhà)."", ""C. Đếm số lượng giá trị bị thiếu."", ""D. So sánh trung bình của nhiều nhóm.""]",B,"[""2.4_KhamPha_TrucQuan""]",easy
Chg2_Q14_MCQ,"Khi dữ liệu có phân phối 'đuôi nặng' (tail-heavy) như thu nhập dân cư, ta thường làm gì để hỗ trợ mô hình học tốt hơn?","[""A. Xóa bỏ phần đuôi."", ""B. Áp dụng biến đổi Log (Log transformation) để dữ liệu trở nên đối xứng hơn (giống phân phối chuẩn)."", ""C. Nhân đôi dữ liệu."", ""D. Không làm gì cả.""]",B,"[""2.5_ChuanBi_DuLieu""]",hard
Chg2_Q15_MCQ,"Kỹ thuật 'One-Hot Encoding' được sử dụng để xử lý loại dữ liệu nào?","[""A. Dữ liệu số liên tục."", ""B. Dữ liệu văn bản dài."", ""C. Dữ liệu phân loại (Categorical) không có thứ tự (ví dụ: màu sắc đỏ, xanh, vàng)."", ""D. Dữ liệu thời gian.""]",C,"[""2.5_ChuanBi_DuLieu""]",medium
Chg2_Q16_MCQ,"Trong Scikit-Learn, vai trò của 'Pipeline' là gì?","[""A. Để vẽ đồ thị dữ liệu."", ""B. Để kết nối chuỗi các bước xử lý dữ liệu (biến đổi) và mô hình thành một luồng duy nhất, giúp code gọn gàng và tránh rò rỉ dữ liệu."", ""C. Để tải dữ liệu từ internet."", ""D. Để lưu mô hình vào ổ cứng.""]",B,"[""2.5_ChuanBi_DuLieu""]",medium
Chg2_Q17_MCQ,"Để lưu lại mô hình đã huấn luyện ra file (để dùng sau này), ta thường dùng thư viện nào trong Python?","[""A. Pandas."", ""B. Matplotlib."", ""C. Joblib hoặc Pickle."", ""D. Numpy.""]",C,"[""2.6_Chon_HuanLuyen_MoHinh""]",easy
Chg2_Q18_MCQ,"Phương pháp 'Ensemble Learning' (Học tập hợp) hoạt động theo nguyên lý nào?","[""A. Sử dụng một mô hình cực kỳ phức tạp."", ""B. Kết hợp dự đoán của nhiều mô hình khác nhau (ví dụ: Random Forest gồm nhiều Decision Trees) để đưa ra kết quả tốt hơn từng mô hình lẻ."", ""C. Chỉ sử dụng dữ liệu huấn luyện tốt nhất."", ""D. Huấn luyện mô hình trong thời gian dài.""]",B,"[""2.7_TinhChinh_MoHinh""]",medium
Chg2_Q19_MCQ,"Tại sao chúng ta không nên chạm vào tập kiểm tra (Test Set) trong suốt quá trình tinh chỉnh tham số (Hyperparameter Tuning)?","[""A. Vì sợ làm hỏng file dữ liệu."", ""B. Để tránh 'Data Snooping' - mô hình sẽ bị tối ưu hóa quá mức cho tập test và mất khả năng tổng quát hóa thực tế."", ""C. Vì tập test cần được giữ bí mật."", ""D. Scikit-Learn không cho phép.""]",B,"[""2.3_LayDuLieu""]",hard
Chg2_Q20_MCQ,"Custom Transformer (Bộ biến đổi tự định nghĩa) trong Scikit-Learn cần cài đặt những phương thức cơ bản nào?","[""A. predict() và score()."", ""B. fit(), transform(), và fit_transform()."", ""C. train() và test()."", ""D. load() và save().""]",B,"[""2.5_ChuanBi_DuLieu""]",hard

Chg2_Q21_MCQ,"Tình huống: Bạn đang làm việc với tập dữ liệu về giá nhà. Thuộc tính 'Vị trí' có các giá trị: 'Gần biển', 'Trung tâm', 'Ngoại ô'. Máy tính không hiểu chữ, bạn nên dùng kỹ thuật nào?","[""A. Min-Max Scaling."", ""B. Imputation."", ""C. One-Hot Encoding (vì các vị trí không có thứ tự hơn kém)."", ""D. Ordinal Encoding (gán 1, 2, 3). ""]",C,"[""2.5_ChuanBi_DuLieu""]",medium
Chg2_Q22_MCQ,"Tình huống: Thuộc tính 'Đánh giá chất lượng' có các giá trị: 'Tệ', 'Trung bình', 'Tốt', 'Xuất sắc'. Bạn nên dùng kỹ thuật mã hóa nào?","[""A. One-Hot Encoding."", ""B. Ordinal Encoding (vì có thứ tự: Tệ < Trung bình < Tốt < Xuất sắc)."", ""C. Xóa bỏ thuộc tính này."", ""D. Feature Scaling.""]",B,"[""2.5_ChuanBi_DuLieu""]",medium
Chg2_Q23_MCQ,"Tình huống: Bạn sử dụng Grid Search để tìm tham số tối ưu nhưng không gian tham số quá lớn (ví dụ: 5 tham số, mỗi tham số thử 10 giá trị). Máy chạy quá lâu. Bạn nên đổi sang chiến lược nào?","[""A. Mua máy tính mới mạnh hơn."", ""B. Randomized Search (tìm kiếm ngẫu nhiên) để khám phá không gian tham số hiệu quả hơn trong giới hạn thời gian."", ""C. Bỏ qua bước tinh chỉnh tham số."", ""D. Giảm dữ liệu huấn luyện xuống còn 1%.""]",B,"[""2.7_TinhChinh_MoHinh""]",medium
Chg2_Q24_MCQ,"Tình huống: Sau khi triển khai mô hình dự báo doanh số, 6 tháng đầu mô hình chạy rất tốt. Sang tháng thứ 7, độ chính xác giảm đột ngột dù code không đổi. Khả năng cao nhất là gì?","[""A. Code bị lỗi theo thời gian."", ""B. Data Drift (Dữ liệu thực tế đã thay đổi hành vi, ví dụ do xu hướng thị trường mới)."", ""C. Máy chủ bị virus."", ""D. Mô hình bị mệt mỏi.""]",B,"[""2.8_TrienKhai_HeThong""]",medium
Chg2_Q25_MCQ,"Tình huống: Mô hình Random Forest của bạn có độ chính xác trên tập Train là 100% nhưng trên tập Validation chỉ là 75%. Bạn đang gặp vấn đề gì và sửa thế nào?","[""A. Underfitting. Cần tăng độ phức tạp mô hình."", ""B. Overfitting. Cần dùng nhiều dữ liệu hơn, giảm độ sâu của cây hoặc tăng số lượng cây."", ""C. Mô hình quá hoàn hảo."", ""D. Do dữ liệu nhiễu.""]",B,"[""2.6_Chon_HuanLuyen_MoHinh""]",hard
Chg2_Q26_MCQ,"Tình huống: Bạn xây dựng một Pipeline gồm: Điền khuyết -> Chuẩn hóa -> Mô hình. Khi dự đoán dữ liệu mới (Test set), bạn cần gọi hàm nào?","[""A. pipeline.fit(test_data)."", ""B. pipeline.fit_transform(test_data)."", ""C. pipeline.predict(test_data) (lưu ý: Pipeline sẽ tự động gọi transform cho dữ liệu test, KHÔNG được fit lại)."", ""D. pipeline.score(test_data).""]",C,"[""2.5_ChuanBi_DuLieu""]",hard
Chg2_Q27_MCQ,"Tình huống: Bạn cần xây dựng hệ thống học máy để phân loại tin tức chứng khoán. Mỗi ngày có hàng nghìn tin mới và bạn muốn hệ thống học liên tục. Kiểu hệ thống nào phù hợp?","[""A. Batch Learning (Học theo lô)."", ""B. Online Learning (Học trực tuyến)."", ""C. Instance-based Learning."", ""D. Unsupervised Learning.""]",B,"[""2.2_BucTranhLon""]",easy

Chg2_Pra_Q3,"Trong đoạn code phân chia dữ liệu: `split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)`, mục đích chính của việc dùng lớp này thay vì `train_test_split` ngẫu nhiên là gì?","[""A. Để chia dữ liệu nhanh hơn."", ""B. Để đảm bảo tập kiểm tra (test set) đại diện chính xác cho tổng thể dựa trên tỷ lệ của một thuộc tính quan trọng (ví dụ: phân bố thu nhập)."", ""C. Để xáo trộn dữ liệu kỹ hơn."", ""D. Để chia dữ liệu thành nhiều phần bằng nhau.""]",B,"[""2.9_ThucHanh_DuAn""]",medium
Chg2_Pra_Q4,"Để xử lý các giá trị bị thiếu (missing values) bằng cách điền vào giá trị trung vị (median) của cột đó, bạn sử dụng câu lệnh nào?","[""A. SimpleImputer(strategy='median')"", ""B. SimpleImputer(strategy='mean')"", ""C. SimpleImputer(strategy='most_frequent')"", ""D. fillna('median') trong Pandas (nhưng không tích hợp được vào Pipeline Scikit-Learn).""]",A,"[""2.9_ThucHanh_DuAn""]",easy
Chg2_Pra_Q5,"Lớp `OneHotEncoder(sparse_output=False)` thường được dùng để xử lý loại dữ liệu nào trong dự án?","[""A. Dữ liệu số (Numerical)."", ""B. Dữ liệu văn bản tự do."", ""C. Dữ liệu phân loại (Categorical) không có thứ tự (ví dụ: 'Ocean_Proximity'), chuyển chúng thành các vector nhị phân."", ""D. Dữ liệu thời gian.""]",C,"[""2.9_ThucHanh_DuAn""]",medium
Chg2_Pra_Q6,"Công cụ `ColumnTransformer` trong Scikit-Learn giải quyết vấn đề thực tế nào khi xây dựng Pipeline?","[""A. Nó giúp vẽ biểu đồ cột."", ""B. Nó cho phép áp dụng các bước tiền xử lý khác nhau cho các cột khác nhau (ví dụ: chuẩn hóa cho cột số, One-hot cho cột chữ) song song trong cùng một quy trình."", ""C. Nó dùng để xóa cột."", ""D. Nó giúp nén dữ liệu.""]",B,"[""2.9_ThucHanh_DuAn""]",hard
Chg2_Pra_Q7,"Tại sao khi sử dụng `cross_val_score` với tham số `scoring='neg_mean_squared_error'`, kết quả trả về lại là các số âm?","[""A. Do dữ liệu bị lỗi."", ""B. Vì quy ước của Scikit-Learn: hàm 'scoring' phải là hàm tiện ích (utility function - càng lớn càng tốt), trong khi MSE là hàm mất mát (cost function - càng nhỏ càng tốt), nên phải đổi dấu."", ""C. Vì RMSE không thể dương."", ""D. Do lỗi của thuật toán Random Forest.""]",B,"[""2.9_ThucHanh_DuAn""]",hard
Chg2_Pra_Q8,"Biến `param_grid` được truyền vào `GridSearchCV` thường có cấu trúc dữ liệu như thế nào?","[""A. Một danh sách các số nguyên."", ""B. Một từ điển (dictionary) ánh xạ tên tham số tới danh sách các giá trị cần thử (ví dụ: `{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6]}`)."", ""C. Một mảng NumPy."", ""D. Một chuỗi văn bản.""]",B,"[""2.9_ThucHanh_DuAn""]",medium
Chg2_Pra_Q9,"Sau khi tìm được mô hình tốt nhất bằng `GridSearchCV`, làm thế nào để truy cập trực tiếp vào mô hình đó?","[""A. grid_search.best_estimator_"", ""B. grid_search.best_params_"", ""C. grid_search.model_"", ""D. grid_search.winner_ ""]",A,"[""2.9_ThucHanh_DuAn""]",easy
Chg2_Pra_Q10,"Thư viện nào được khuyên dùng trong sách để lưu (save) và tải (load) mô hình Scikit-Learn một cách hiệu quả (đặc biệt với các mảng NumPy lớn)?","[""A. pickle"", ""B. joblib"", ""C. json"", ""D. csv""]",B,"[""2.9_ThucHanh_DuAn""]",easy

Chg3_Q1_MCQ,"Bộ dữ liệu MNIST thường được gọi là gì trong lĩnh vực Machine Learning?","[""A. 'Hello World' của Machine Learning."", ""B. Bộ dữ liệu khó nhất thế giới."", ""C. Bộ dữ liệu về giá nhà."", ""D. Bộ dữ liệu khuôn mặt người.""]",A,"[""3.1_MNIST""]",easy
Chg3_Q2_MCQ,"Bộ phân loại nhị phân (Binary Classifier) có đặc điểm gì?","[""A. Phân loại dữ liệu thành nhiều hơn 2 lớp."", ""B. Chỉ phân biệt giữa hai lớp (ví dụ: 'Số 5' và 'Không phải số 5')."", ""C. Luôn luôn đúng 100%."", ""D. Không cần huấn luyện.""]",B,"[""3.2_PhanLoai_NhiPhan""]",easy
Chg3_Q3_MCQ,"Ma trận nhầm lẫn (Confusion Matrix) giúp đánh giá điều gì?","[""A. Tốc độ của thuật toán."", ""B. Hiệu suất chi tiết của mô hình phân loại (biết được mô hình hay nhầm lẫn lớp nào với lớp nào)."", ""C. Kích thước của dữ liệu."", ""D. Giá trị dự đoán của hồi quy.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",easy
Chg3_Q4_MCQ,"Độ chính xác (Precision) được tính theo công thức nào?","[""A. TP / (TP + FN)"", ""B. TP / (TP + FP) (Tỷ lệ dự đoán đúng trong số các trường hợp được dự đoán là dương tính)."", ""C. (TP + TN) / Tổng số"", ""D. FP / (TN + FP)""]",B,"[""3.3_DoHieuQua_PhanLoai""]",medium
Chg3_Q5_MCQ,"Độ phủ (Recall) hay Độ nhạy (Sensitivity) quan trọng trong trường hợp nào?","[""A. Khi muốn chắc chắn rằng dự đoán dương tính là cực kỳ chính xác."", ""B. Khi chi phí bỏ sót một trường hợp dương tính là rất cao (ví dụ: phát hiện ung thư)."", ""C. Khi dữ liệu cân bằng tuyệt đối."", ""D. Khi không quan tâm đến False Negatives.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",medium
Chg3_Q6_MCQ,"Chiến lược 'One-versus-All' (OvA) hay 'One-versus-the-Rest' dùng để làm gì?","[""A. Để so sánh các mô hình với nhau."", ""B. Để giải quyết bài toán phân loại đa lớp bằng cách sử dụng nhiều bộ phân loại nhị phân."", ""C. Để giảm chiều dữ liệu."", ""D. Để chọn ra đặc trưng tốt nhất.""]",B,"[""3.4_PhanLoai_DaLop""]",hard
Chg3_Q7_MCQ,"Khi phân tích lỗi (Error Analysis) của một mô hình phân loại, việc xem xét Confusion Matrix dưới dạng hình ảnh (đồ thị nhiệt) giúp ích gì?","[""A. Làm cho báo cáo đẹp hơn."", ""B. Dễ dàng nhận ra các mẫu lỗi sai cụ thể (ví dụ: số 3 hay bị nhầm thành số 5)."", ""C. Tăng độ chính xác ngay lập tức."", ""D. Giảm thời gian huấn luyện.""]",B,"[""3.5_PhanTich_Loi""]",medium
Chg3_Q8_MCQ,"Hệ thống phân loại đa nhãn (Multilabel Classification) là gì?","[""A. Phân loại đối tượng vào một lớp duy nhất trong nhiều lớp."", ""B. Mỗi đối tượng có thể được gán nhiều nhãn cùng lúc (ví dụ: một bức ảnh có cả 'người' và 'xe đạp')."", ""C. Hệ thống có nhiều người dán nhãn dữ liệu."", ""D. Phân loại dựa trên nhãn mác sản phẩm.""]",B,"[""3.6_PhanLoai_DaNhan""]",hard

Chg3_Q9_MCQ,"Tại sao 'Độ chính xác' (Accuracy) không phải là thước đo tốt cho các bộ dữ liệu bị lệch (Skewed Datasets)?","[""A. Vì nó tính toán quá chậm."", ""B. Vì nếu dữ liệu có 90% là lớp A, một mô hình ngây thơ luôn đoán là A cũng đã đạt độ chính xác 90%, dù nó không học được gì."", ""C. Vì Accuracy chỉ dùng cho hồi quy."", ""D. Vì Scikit-Learn không hỗ trợ.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",medium
Chg3_Q10_MCQ,"Điểm F1 (F1 Score) là gì?","[""A. Trung bình cộng của Precision và Recall."", ""B. Trung bình điều hòa (Harmonic Mean) của Precision và Recall, giúp trừng phạt các giá trị cực thấp."", ""C. Hiệu số giữa Precision và Recall."", ""D. Tổng của True Positive và True Negative.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",medium
Chg3_Q11_MCQ,"Mối quan hệ đánh đổi (Trade-off) giữa Precision và Recall diễn ra như thế nào?","[""A. Khi tăng Precision thì Recall cũng tăng theo."", ""B. Khi tăng ngưỡng quyết định (Decision Threshold) để tăng Precision, Recall thường sẽ giảm xuống (và ngược lại)."", ""C. Không có mối liên hệ nào giữa hai chỉ số này."", ""D. Precision và Recall luôn bằng nhau.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",hard
Chg3_Q12_MCQ,"Đường cong ROC (Receiver Operating Characteristic) biểu diễn mối quan hệ giữa hai đại lượng nào?","[""A. Precision và Recall."", ""B. Tỷ lệ dương tính thật (TPR - Recall) và Tỷ lệ dương tính giả (FPR)."", ""C. Accuracy và Loss."", ""D. F1 Score và Threshold.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",hard
Chg3_Q13_MCQ,"Diện tích dưới đường cong ROC (AUC - Area Under Curve) bằng 0.5 thể hiện điều gì?","[""A. Mô hình hoàn hảo."", ""B. Mô hình dự đoán ngẫu nhiên (như tung đồng xu), không có khả năng phân loại."", ""C. Mô hình bị Overfitting."", ""D. Mô hình bị lỗi code.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",medium
Chg3_Q14_MCQ,"Bạn nên ưu tiên sử dụng đường cong PR (Precision-Recall) thay vì ROC khi nào?","[""A. Khi dữ liệu rất cân bằng."", ""B. Khi lớp dương tính (Positive class) rất hiếm hoặc khi bạn quan tâm đến False Positives hơn là False Negatives."", ""C. Khi bạn không quan tâm đến kết quả."", ""D. Khi sử dụng thuật toán Random Forest.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",hard

Chg3_Q15_MCQ,"Chiến lược 'One-versus-One' (OvO) khác 'One-versus-All' (OvA) ở điểm nào?","[""A. OvO huấn luyện ít bộ phân loại hơn OvA."", ""B. OvO huấn luyện một bộ phân loại cho MỖI CẶP lớp (ví dụ: 0 vs 1, 0 vs 2...), phù hợp với thuật toán kém hiệu quả trên dữ liệu lớn (như SVM)."", ""C. OvO chỉ dùng cho phân loại nhị phân."", ""D. OvO không tồn tại trong thực tế.""]",B,"[""3.4_PhanLoai_DaLop""]",hard
Chg3_Q16_MCQ,"Bộ phân loại SGD (Stochastic Gradient Descent) trong Scikit-Learn có ưu điểm gì nổi bật?","[""A. Luôn luôn chính xác 100%."", ""B. Xử lý hiệu quả các bộ dữ liệu rất lớn và hỗ trợ học trực tuyến (Online Learning)."", ""C. Không cần tinh chỉnh siêu tham số."", ""D. Chỉ dùng được cho dữ liệu ảnh.""]",B,"[""3.2_PhanLoai_NhiPhan""]",medium
Chg3_Q17_MCQ,"Phân loại đa đầu ra (Multioutput Classification) là gì? Hãy chọn ví dụ đúng.","[""A. Phân loại chó và mèo."", ""B. Làm sạch nhiễu khỏi ảnh (Input: ảnh nhiễu, Output: ảnh sạch - trong đó mỗi pixel là một nhãn giá trị cường độ sáng)."", ""C. Dự đoán giá nhà."", ""D. Gom nhóm khách hàng.""]",B,"[""3.6_PhanLoai_DaNhan""]",hard
Chg3_Q18_MCQ,"Tình huống: Bạn xây dựng bộ lọc video an toàn cho trẻ em (Kids Safe Filter). Bạn sẽ ưu tiên chỉ số nào?","[""A. Recall cao (chấp nhận giữ lại video xấu còn hơn bỏ sót video tốt)."", ""B. Precision cao (thà loại bỏ nhầm video tốt còn hơn để lọt video xấu)."", ""C. Chỉ cần Accuracy cao."", ""D. F1 Score trung bình.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",medium
Chg3_Q19_MCQ,"Tình huống: Bạn xây dựng hệ thống phát hiện kẻ trộm trong siêu thị qua camera. Bạn muốn bắt được càng nhiều kẻ trộm càng tốt, dù đôi khi báo nhầm người ngay thẳng. Bạn cần tối ưu chỉ số nào?","[""A. Precision."", ""B. Recall (Độ phủ)."", ""C. Specificity."", ""D. Confusion Matrix.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",medium
Chg3_Q20_MCQ,"Khi nhìn vào Confusion Matrix, các con số nằm trên 'đường chéo chính' (từ góc trên trái xuống góc dưới phải) đại diện cho điều gì?","[""A. Số lượng dự đoán sai."", ""B. Số lượng dự đoán đúng (True Positives và True Negatives)."", ""C. Tổng số mẫu dữ liệu."", ""D. Các lớp không quan trọng.""]",B,"[""3.3_DoHieuQua_PhanLoai""]",easy

Chg3_Pra_Q3,"Trong Scikit-Learn, tại sao ta thường dùng `cross_val_predict()` thay vì `cross_val_score()` khi muốn xây dựng Ma trận nhầm lẫn (Confusion Matrix)?","[""A. Vì `cross_val_score` chỉ trả về điểm số đánh giá (như accuracy) chứ không trả về nhãn dự đoán cụ thể cho từng mẫu dữ liệu."", ""B. Vì `cross_val_predict` chạy nhanh hơn."", ""C. Vì `cross_val_score` bị lỗi với dữ liệu ảnh."", ""D. Vì `cross_val_predict` tự động vẽ biểu đồ.""]",A,"[""3.7_ThucHanh_PhanLoai""]",medium
Chg3_Pra_Q4,"Khi gọi hàm `confusion_matrix(y_true, y_pred)`, thứ tự tham số có quan trọng không?","[""A. Không, Scikit-Learn tự động nhận diện."", ""B. Có, tham số đầu tiên phải là nhãn thực tế (y_true), tham số thứ hai là nhãn dự đoán (y_pred). Nếu đảo ngược, ma trận sẽ bị chuyển vị (transposed) gây hiểu nhầm."", ""C. Có, tham số đầu tiên phải là y_pred."", ""D. Chỉ quan trọng nếu là phân loại đa lớp.""]",B,"[""3.7_ThucHanh_PhanLoai""]",easy
Chg3_Pra_Q5,"Để vẽ đường cong Precision-Recall, bạn cần lấy được 'điểm số quyết định' (decision scores) thay vì nhãn dự đoán. Bạn làm điều này bằng cách nào với `SGDClassifier`?","[""A. Gọi `model.predict(X)`."", ""B. Gọi `model.decision_function(X)` (hoặc dùng `method='decision_function'` trong `cross_val_predict`)."", ""C. Gọi `model.score(X)`."", ""D. Gọi `model.probability(X)`.""]",B,"[""3.7_ThucHanh_PhanLoai""]",hard
Chg3_Pra_Q6,"Thuật toán `SGDClassifier` rất nhạy cảm với tỷ lệ (scale) của các đặc trưng. Bước tiền xử lý nào là BẮT BUỘC để nó hoạt động hiệu quả?","[""A. `MinMaxScaler`."", ""B. `StandardScaler` (để đưa dữ liệu về phân phối chuẩn với trung bình 0, phương sai 1)."", ""C. `Normalizer`."", ""D. Không cần xử lý gì cả.""]",B,"[""3.7_ThucHanh_PhanLoai""]",medium
Chg3_Pra_Q7,"Để thực hiện bài toán Phân loại đa nhãn (Multilabel Classification), ví dụ một bức ảnh vừa là 'số lớn' (>=7) vừa là 'số lẻ', thuật toán nào trong Scikit-Learn hỗ trợ tự nhiên (native support)?","[""A. `SGDClassifier`."", ""B. `KNeighborsClassifier`."", ""C. `LogisticRegression`."", ""D. `SVC`.""]",B,"[""3.7_ThucHanh_PhanLoai""]",hard
Chg3_Pra_Q8,"Hàm `plot_precision_recall_curve` (hoặc `PrecisionRecallDisplay`) yêu cầu đầu vào là gì để vẽ đồ thị?","[""A. Chỉ cần model và dữ liệu X, y."", ""B. Cần danh sách các ngưỡng (thresholds)."", ""C. Cần Confusion Matrix."", ""D. Cần F1 Score.""]",A,"[""3.7_ThucHanh_PhanLoai""]",medium
Chg3_Pra_Q9,"Khi phân tích lỗi (Error Analysis) cho bài toán phân loại đa lớp, việc chuẩn hóa ma trận nhầm lẫn bằng cách chia mỗi giá trị cho tổng số ảnh của lớp tương ứng (hàng) giúp ích gì?","[""A. Giúp ma trận đẹp hơn."", ""B. Giúp so sánh tỷ lệ lỗi sai tương đối giữa các lớp, tránh việc các lớp có nhiều dữ liệu (lớp lớn) trông có vẻ bị sai nhiều hơn các lớp ít dữ liệu."", ""C. Giúp chuyển đổi sang ảnh đen trắng."", ""D. Giúp tăng độ chính xác.""]",B,"[""3.7_ThucHanh_PhanLoai""]",hard
Chg3_Pra_Q10,"Bộ dữ liệu MNIST có thể được tải về dễ dàng bằng hàm nào của Scikit-Learn?","[""A. `load_mnist()`"", ""B. `fetch_openml('mnist_784', as_frame=False)`"", ""C. `get_data('mnist')`"", ""D. `import_dataset('mnist')`""]",B,"[""3.7_ThucHanh_PhanLoai""]",easy

Chg4_Q1_MCQ,"Phương trình pháp tuyến (Normal Equation) trong Hồi quy tuyến tính dùng để làm gì?","[""A. Để tìm ra bộ tham số mô hình tốt nhất (Minimize Cost Function) bằng công thức toán học giải tích trực tiếp."", ""B. Để chuẩn hóa dữ liệu."", ""C. Để thực hiện Gradient Descent."", ""D. Để vẽ đồ thị.""]",A,"[""4.1_HoiQuy_TuyenTinh""]",easy
Chg4_Q2_MCQ,"Nhược điểm lớn nhất của Phương trình pháp tuyến (Normal Equation) là gì?","[""A. Không chính xác."", ""B. Độ phức tạp tính toán rất cao khi số lượng đặc trưng (features) lớn (ví dụ: n > 100,000), do phải tính nghịch đảo ma trận."", ""C. Không hoạt động với số thực."", ""D. Chỉ hoạt động trên dữ liệu ảnh.""]",B,"[""4.1_HoiQuy_TuyenTinh""]",hard
Chg4_Q3_MCQ,"Trong thuật toán Gradient Descent, tham số 'Learning Rate' (Tốc độ học) quá cao sẽ dẫn đến hiện tượng gì?","[""A. Mô hình hội tụ rất chậm."", ""B. Mô hình hội tụ ngay lập tức."", ""C. Thuật toán có thể nhảy qua điểm tối ưu và phân kỳ (Diverge) - càng chạy càng sai."", ""D. Không ảnh hưởng gì.""]",C,"[""4.2_Gradient_Descent""]",medium
Chg4_Q4_MCQ,"Tại sao cần phải thực hiện Feature Scaling (Chuẩn hóa đặc trưng) trước khi chạy Gradient Descent?","[""A. Để mô hình đẹp hơn."", ""B. Để thuật toán hội tụ nhanh hơn (đường đi xuống dốc thẳng hơn thay vì đi zíc-zắc)."", ""C. Phương trình pháp tuyến yêu cầu bắt buộc."", ""D. Để biến đổi bài toán thành phân loại.""]",B,"[""4.2_Gradient_Descent""]",medium
Chg4_Q5_MCQ,"Sự khác biệt chính giữa Batch Gradient Descent và Stochastic Gradient Descent (SGD) là gì?","[""A. Batch GD dùng toàn bộ dữ liệu để tính toán mỗi bước (chậm nhưng ổn định), còn SGD dùng duy nhất 1 mẫu ngẫu nhiên (nhanh nhưng dao động mạnh)."", ""B. Batch GD dùng cho RAM nhỏ, SGD dùng cho RAM lớn."", ""C. Batch GD luôn tốt hơn SGD."", ""D. SGD không bao giờ hội tụ.""]",A,"[""4.2_Gradient_Descent""]",medium
Chg4_Q6_MCQ,"Mini-batch Gradient Descent là sự kết hợp giữa?","[""A. Linear Regression và Logistic Regression."", ""B. Batch GD và Stochastic GD (Lấy một nhóm nhỏ mẫu ngẫu nhiên thay vì 1 hoặc tất cả)."", ""C. Học có giám sát và không giám sát."", ""D. Normal Equation và SVD.""]",B,"[""4.2_Gradient_Descent""]",easy
Chg4_Q7_MCQ,"Đồ thị hàm mất mát (Loss Function) của Hồi quy tuyến tính (MSE) có hình dạng gì đặc biệt?","[""A. Hình sin với nhiều cực tiểu cục bộ (Local Minima)."", ""B. Hình cái bát (Convex/Lồi), đảm bảo chỉ có một cực tiểu toàn cục (Global Minimum) duy nhất."", ""C. Hình phẳng."", ""D. Hình yên ngựa.""]",B,"[""4.2_Gradient_Descent""]",medium
Chg4_Q8_MCQ,"Hồi quy đa thức (Polynomial Regression) thực chất là gì?","[""A. Một thuật toán hoàn toàn mới không liên quan đến hồi quy tuyến tính."", ""B. Vẫn là hồi quy tuyến tính, nhưng ta tạo thêm các đặc trưng mới bằng cách lũy thừa các đặc trưng cũ (x^2, x^3...) rồi đưa vào mô hình."", ""C. Dùng để phân loại ảnh."", ""D. Là thuật toán cây quyết định.""]",B,"[""4.3_HoiQuy_DaThuc""]",medium
Chg4_Q9_MCQ,"Khi sử dụng Hồi quy đa thức bậc cao (ví dụ bậc 100), nguy cơ lớn nhất là gì?","[""A. Underfitting (Mô hình quá đơn giản)."", ""B. Overfitting (Mô hình quá phức tạp, uốn lượn theo từng điểm dữ liệu nhiễu)."", ""C. Máy tính không tính được."", ""D. Dữ liệu bị xóa mất.""]",B,"[""4.3_HoiQuy_DaThuc""]",medium

Chg4_Q10_MCQ,"Khi quan sát Learning Curve (Đường cong học tập), dấu hiệu nào cho thấy mô hình bị 'Underfitting' (Thiếu khớp)?","[""A. Sai số trên tập Train rất thấp, nhưng trên tập Validation rất cao."", ""B. Sai số trên cả tập Train và tập Validation đều cao và gần bằng nhau."", ""C. Sai số trên tập Train và Validation đều thấp."", ""D. Đường sai số Validation dao động mạnh.""]",B,"[""4.4_Learning_Curves""]",medium
Chg4_Q11_MCQ,"Nếu mô hình bị 'Overfitting' (Quá khớp), Learning Curve thường trông như thế nào?","[""A. Hai đường Train và Validation nằm sát nhau ở mức lỗi cao."", ""B. Có một khoảng cách (gap) lớn giữa đường Train (lỗi thấp) và đường Validation (lỗi cao hơn nhiều)."", ""C. Hai đường hội tụ về 0."", ""D. Đường Validation thấp hơn đường Train.""]",B,"[""4.4_Learning_Curves""]",medium
Chg4_Q12_MCQ,"Ridge Regression (Hồi quy Ridge) khác Hồi quy tuyến tính thường ở điểm nào?","[""A. Nó thêm một khoản phạt (penalty) tỷ lệ với bình phương độ lớn của trọng số (L2 norm) vào hàm mất mát, giúp giữ các trọng số nhỏ nhất có thể."", ""B. Nó sử dụng cây quyết định."", ""C. Nó loại bỏ các đặc trưng không quan trọng."", ""D. Nó chỉ dùng cho bài toán phân loại.""]",A,"[""4.5_MoHinh_ChinhQuy""]",medium
Chg4_Q13_MCQ,"Đặc điểm nổi bật nhất của Lasso Regression (L1 Regularization) là gì?","[""A. Luôn cho kết quả chính xác hơn Ridge."", ""B. Có khả năng tự động loại bỏ các đặc trưng ít quan trọng (đưa trọng số về đúng bằng 0), giúp mô hình thưa (sparse) và dễ giải thích hơn."", ""C. Tính toán nhanh hơn Ridge."", ""D. Không bao giờ bị Overfitting.""]",B,"[""4.5_MoHinh_ChinhQuy""]",hard
Chg4_Q14_MCQ,"Kỹ thuật 'Early Stopping' (Dừng sớm) hoạt động như thế nào?","[""A. Dừng huấn luyện ngay khi sai số trên tập Train bắt đầu tăng."", ""B. Dừng huấn luyện ngay khi sai số trên tập Validation bắt đầu tăng trở lại (dấu hiệu mô hình bắt đầu Overfit)."", ""C. Dừng huấn luyện sau đúng 100 vòng lặp."", ""D. Dừng khi máy tính quá nóng.""]",B,"[""4.5_MoHinh_ChinhQuy""]",easy
Chg4_Q15_MCQ,"Elastic Net là sự kết hợp giữa hai phương pháp nào?","[""A. Batch GD và Stochastic GD."", ""B. Ridge Regression và Lasso Regression."", ""C. Linear Regression và Logistic Regression."", ""D. SVM và Decision Tree.""]",B,"[""4.5_MoHinh_ChinhQuy""]",easy
Chg4_Q16_MCQ,"Hồi quy Logistic (Logistic Regression) thường được dùng cho bài toán nào?","[""A. Dự đoán giá nhà (Hồi quy)."", ""B. Phân loại nhị phân (Binary Classification), ví dụ: Thư rác hay không."", ""C. Phân cụm khách hàng."", ""D. Tạo ảnh nghệ thuật.""]",B,"[""4.6_HoiQuy_Logistic""]",easy
Chg4_Q17_MCQ,"Hàm Sigmoid trong Hồi quy Logistic có tác dụng gì?","[""A. Biến đổi đầu ra thành một giá trị xác suất nằm trong khoảng [0, 1]."", ""B. Biến đổi đầu ra thành số nguyên."", ""C. Loại bỏ số âm."", ""D. Tăng tốc độ tính toán.""]",A,"[""4.6_HoiQuy_Logistic""]",medium
Chg4_Q18_MCQ,"Hồi quy Softmax (Softmax Regression) là phiên bản mở rộng của Hồi quy Logistic để giải quyết vấn đề gì?","[""A. Phân loại đa lớp (Multiclass Classification) mà các lớp loại trừ lẫn nhau (ví dụ: nhận diện số 0-9)."", ""B. Phân loại đa nhãn (Multilabel)."", ""C. Hồi quy đa biến."", ""D. Xử lý dữ liệu chuỗi.""]",A,"[""4.6_HoiQuy_Logistic""]",medium
Chg4_Q19_MCQ,"Hàm mất mát (Cost Function) thường dùng cho Hồi quy Logistic và Softmax là gì?","[""A. Mean Squared Error (MSE)."", ""B. Mean Absolute Error (MAE)."", ""C. Log Loss (Cross-Entropy Loss)."", ""D. Hinge Loss.""]",C,"[""4.6_HoiQuy_Logistic""]",hard
Chg4_Q20_MCQ,"Tại sao khi dùng Gradient Descent, ta thích dùng hàm Log Loss (lồi) hơn là MSE cho bài toán phân loại?","[""A. Vì nó tính nhanh hơn."", ""B. Vì hàm Log Loss đảm bảo là hàm lồi (Convex), giúp Gradient Descent luôn tìm được cực tiểu toàn cục (Global Minimum) mà không bị kẹt ở cực tiểu địa phương."", ""C. Vì MSE không dùng được cho số 0 và 1."", ""D. Không có lý do gì đặc biệt.""]",B,"[""4.6_HoiQuy_Logistic""]",hard

Chg4_Pra_Q3,"Để thực hiện Hồi quy đa thức (Polynomial Regression) bậc 2, bước đầu tiên là biến đổi dữ liệu bằng class nào?","[""A. PolynomialFeatures(degree=2, include_bias=False)"", ""B. LinearRegression(degree=2)"", ""C. PolynomialRegression(degree=2)"", ""D. StandardScaler() ""]",A,"[""4.7_ThucHanh_HuanLuyen""]",medium
Chg4_Pra_Q4,"Sau khi huấn luyện mô hình `lin_reg = LinearRegression()`, làm thế nào để truy cập vào giá trị hệ số góc (weights) và hệ số chặn (bias/intercept)?","[""A. lin_reg.weights và lin_reg.bias"", ""B. lin_reg.coef_ và lin_reg.intercept_"", ""C. lin_reg.w và lin_reg.b"", ""D. lin_reg.params_ ""]",B,"[""4.7_ThucHanh_HuanLuyen""]",easy
Chg4_Pra_Q5,"Khi sử dụng `SGDRegressor`, tham số `penalty='l2'` tương đương với loại hồi quy nào?","[""A. Lasso Regression"", ""B. Ridge Regression"", ""C. Elastic Net"", ""D. Linear Regression thường""]",B,"[""4.7_ThucHanh_HuanLuyen""]",medium
Chg4_Pra_Q6,"Để triển khai kỹ thuật 'Early Stopping' thủ công (tự viết vòng lặp huấn luyện và dừng khi lỗi không giảm), bạn cần cài đặt tham số nào cho `SGDRegressor` để mô hình giữ lại trạng thái huấn luyện sau mỗi lần gọi `.fit()`?","[""A. early_stopping=True"", ""B. warm_start=True"", ""C. keep_weights=True"", ""D. partial_fit=True""]",B,"[""4.7_ThucHanh_HuanLuyen""]",hard
Chg4_Pra_Q7,"Trong `LogisticRegression`, để lấy ra xác suất dự đoán (ví dụ: 70% là thư rác) thay vì nhãn lớp (0 hoặc 1), ta dùng phương thức nào?","[""A. predict()"", ""B. predict_proba()"", ""C. predict_probability()"", ""D. decision_function()""]",B,"[""4.7_ThucHanh_HuanLuyen""]",easy
Chg4_Pra_Q8,"Để cấu hình `LogisticRegression` hoạt động như một bộ phân loại Softmax (Softmax Regression) cho bài toán đa lớp, bạn cần thiết lập các tham số nào?","[""A. multi_class='multinomial', solver='lbfgs'"", ""B. multi_class='ovr', solver='liblinear'"", ""C. activation='softmax'"", ""D. loss='categorical_crossentropy'""]",A,"[""4.7_ThucHanh_HuanLuyen""]",hard
Chg4_Pra_Q9,"Hàm `learning_curve` trong `sklearn.model_selection` trả về 3 giá trị chính, đó là gì?","[""A. train_sizes, train_scores, test_scores"", ""B. train_loss, val_loss, epochs"", ""C. estimator, X, y"", ""D. precision, recall, thresholds""]",A,"[""4.7_ThucHanh_HuanLuyen""]",medium
Chg4_Pra_Q10,"Khi sử dụng `Ridge(alpha=1, solver='cholesky')`, tham số `alpha` đóng vai trò gì?","[""A. Tốc độ học (Learning Rate)."", ""B. Cường độ điều chuẩn (Regularization strength): giá trị càng lớn thì trọng số càng bị ép nhỏ lại (giảm Overfitting)."", ""C. Bậc của đa thức."", ""D. Số lượng vòng lặp.""]",B,"[""4.7_ThucHanh_HuanLuyen""]",medium

Chg5_Q1_MCQ,"Ý tưởng cốt lõi của thuật toán SVM (Support Vector Machine) là gì?","[""A. Tìm một đường thẳng (hoặc siêu phẳng) phân chia các lớp sao cho khoảng cách (lề đường - margin) giữa đường này và các điểm dữ liệu gần nhất là lớn nhất."", ""B. Tìm đường đi qua tất cả các điểm dữ liệu."", ""C. Tìm trọng tâm của các cụm dữ liệu."", ""D. Tạo ra nhiều cây quyết định.""]",A,"[""5.1_SVM_TuyenTinh""]",easy
Chg5_Q2_MCQ,"Tại sao thuật toán SVM lại rất nhạy cảm với Feature Scaling (Chuẩn hóa dữ liệu)?","[""A. Vì SVM tính toán khoảng cách; nếu một đặc trưng có giá trị quá lớn so với đặc trưng kia, nó sẽ chi phối hoàn toàn việc tính khoảng cách và làm méo 'lề đường'."", ""B. Vì SVM chạy chậm trên số lớn."", ""C. Vì SVM không xử lý được số âm."", ""D. Không, SVM không cần chuẩn hóa.""]",A,"[""5.1_SVM_TuyenTinh""]",medium
Chg5_Q3_MCQ,"Support Vectors (Vectơ hỗ trợ) là những điểm nào?","[""A. Tất cả các điểm dữ liệu trong tập huấn luyện."", ""B. Chỉ những điểm nằm ngoài lề đường (margin)."", ""C. Chỉ những điểm nằm ngay trên mép của lề đường (hoặc nằm sai phía trong lề đường), chúng là những điểm duy nhất quyết định hình dáng của ranh giới quyết định."", ""D. Trọng tâm của các lớp.""]",C,"[""5.4_CoChe_HoatDong""]",medium
Chg5_Q4_MCQ,"Tham số C trong SVM kiểm soát điều gì (Hard Margin vs Soft Margin)?","[""A. C càng lớn, lề đường càng rộng, chấp nhận nhiều sai số (Underfitting)."", ""B. C càng lớn, lề đường càng hẹp, cố gắng phân loại đúng mọi điểm (nguy cơ Overfitting). C càng nhỏ, lề đường càng rộng (Regularization)."", ""C. C là số lượng Kernel."", ""D. C là tốc độ học.""]",B,"[""5.1_SVM_TuyenTinh""]",hard
Chg5_Q5_MCQ,"Nếu dữ liệu huấn luyện có chứa điểm ngoại lai (outliers) khiến không thể tìm được đường phân chia tuyến tính (Hard Margin Impossible), bạn nên làm gì?","[""A. Dùng Hard Margin Classification."", ""B. Dùng Soft Margin Classification (giảm C) để cho phép một số vi phạm lề đường."", ""C. Bỏ bài toán này."", ""D. Tăng Learning Rate.""]",B,"[""5.1_SVM_TuyenTinh""]",easy
Chg5_Q6_MCQ,"'Kernel Trick' (Thủ thuật hạt nhân) giúp giải quyết vấn đề gì?","[""A. Giúp SVM hoạt động trên dữ liệu phi tuyến (Non-linear) bằng cách ánh xạ dữ liệu sang không gian số chiều cao hơn mà không cần tính toán tọa độ thực sự, tiết kiệm chi phí tính toán."", ""B. Giúp giảm số chiều dữ liệu."", ""C. Giúp SVM chạy nhanh hơn trên dữ liệu tuyến tính."", ""D. Giúp SVM xử lý dữ liệu bị thiếu.""]",A,"[""5.2_SVM_PhiTuyen""]",hard
Chg5_Q7_MCQ,"Trong RBF Kernel (Gaussian RBF), tham số Gamma ảnh hưởng thế nào?","[""A. Gamma càng cao, đường ranh giới càng uốn lượn phức tạp bao quanh từng điểm dữ liệu (Overfitting). Gamma càng thấp, đường ranh giới càng mượt (Underfitting)."", ""B. Gamma càng cao, mô hình càng đơn giản."", ""C. Gamma không ảnh hưởng gì."", ""D. Gamma là số lượng cây quyết định.""]",A,"[""5.2_SVM_PhiTuyen""]",hard
Chg5_Q8_MCQ,"Sự khác biệt giữa SVM Phân loại (Classification) và SVM Hồi quy (Regression - SVR) là gì?","[""A. SVM Phân loại cố gắng đưa điểm ra xa khỏi lề đường, còn SVR cố gắng đưa càng nhiều điểm vào BÊN TRONG lề đường càng tốt."", ""B. SVM Phân loại dùng cho số thực, SVR dùng cho nhãn."", ""C. SVR không có tham số C."", ""D. SVR chạy nhanh hơn.""]",A,"[""5.3_SVM_HoiQuy""]",medium
Chg5_Q9_MCQ,"Trong SVM Hồi quy, tham số Epsilon (ε) quyết định điều gì?","[""A. Số lượng đặc trưng."", ""B. Độ rộng của 'lề đường' (ống). Các điểm nằm trong ống này sẽ không bị tính vào hàm mất mát (Epsilon-insensitive)."", ""C. Tốc độ học."", ""D. Số lượng Support Vectors.""]",B,"[""5.3_SVM_HoiQuy""]",medium
Chg5_Q10_MCQ,"Tại sao SVM thường không được khuyên dùng cho các bộ dữ liệu rất lớn (ví dụ hàng triệu mẫu)?","[""A. Vì nó chiếm quá nhiều RAM và thời gian huấn luyện tăng theo hàm bậc 2 hoặc bậc 3 của số lượng mẫu (O(m^2) đến O(m^3))."", ""B. Vì nó dự đoán quá chậm."", ""C. Vì độ chính xác thấp."", ""D. Vì Scikit-Learn không hỗ trợ.""]",A,"[""5.4_CoChe_HoatDong""]",medium
Chg5_Q11_MCQ,"Hàm mất mát (Loss Function) được sử dụng trong SVM là gì?","[""A. Mean Squared Error."", ""B. Cross-Entropy Loss."", ""C. Hinge Loss (max(0, 1 - t*y))."", ""D. Log Loss.""]",C,"[""5.4_CoChe_HoatDong""]",hard

Chg5_Pra_Q3,"Khi làm việc với bộ dữ liệu rất lớn (hàng trăm ngàn mẫu) cho bài toán phân loại tuyến tính, class nào được khuyên dùng để tối ưu tốc độ (dựa trên thư viện `liblinear`)?","[""A. SVC(kernel='linear')"", ""B. LinearSVC()"", ""C. SVC(kernel='poly')"", ""D. NuSVC() ""]",B,"[""5.5_ThucHanh_SVM""]",medium
Chg5_Pra_Q4,"Để sử dụng thuật toán SVM với Stochastic Gradient Descent (hữu ích cho Online Learning hoặc dữ liệu khổng lồ), bạn cần khởi tạo `SGDClassifier` với tham số `loss` là gì?","[""A. loss='log_loss'"", ""B. loss='hinge'"", ""C. loss='squared_error'"", ""D. loss='perceptron' ""]",B,"[""5.5_ThucHanh_SVM""]",hard
Chg5_Pra_Q5,"Trong `SVC(kernel='poly')`, tham số `coef0` đóng vai trò gì?","[""A. Nó kiểm soát mức độ ảnh hưởng của các đa thức bậc cao so với đa thức bậc thấp (giúp tinh chỉnh mô hình)."", ""B. Nó là hệ số chặn (bias)."", ""C. Nó là số lượng support vectors."", ""D. Nó quy định số chiều dữ liệu.""]",A,"[""5.5_ThucHanh_SVM""]",hard
Chg5_Pra_Q6,"Nếu bạn muốn sử dụng `SVC` để dự đoán xác suất (thông qua phương thức `predict_proba()`), bạn BẮT BUỘC phải thiết lập tham số nào khi khởi tạo (lưu ý điều này sẽ làm chậm quá trình huấn luyện)?","[""A. probability=True"", ""B. predict_proba=True"", ""C. score=True"", ""D. method='sigmoid' ""]",A,"[""5.5_ThucHanh_SVM""]",medium
Chg5_Pra_Q7,"Đối với bài toán Hồi quy (Regression), class `LinearSVR` có tham số `epsilon` (ε). Tham số này định nghĩa điều gì?","[""A. Sai số tối đa cho phép."", ""B. Độ rộng của 'ống' (margin) mà trong đó các điểm dữ liệu KHÔNG bị tính vào hàm mất mát (không bị phạt)."", ""C. Số lượng đặc trưng."", ""D. Tốc độ học.""]",B,"[""5.5_ThucHanh_SVM""]",medium
Chg5_Pra_Q8,"Mô hình SVM rất nhạy cảm với tỷ lệ của dữ liệu. Bạn nên dùng Transformer nào trong Pipeline trước khi đưa dữ liệu vào SVM?","[""A. MinMaxScaler()"", ""B. StandardScaler() (để đưa về trung bình 0, phương sai 1)."", ""C. Normalizer()"", ""D. PolynomialFeatures() ""]",B,"[""5.5_ThucHanh_SVM""]",easy
Chg5_Pra_Q9,"Khi sử dụng hạt nhân RBF (`kernel='rbf'`), nếu mô hình bị Overfitting (Quá khớp), bạn nên điều chỉnh `gamma` và `C` như thế nào?","[""A. Tăng gamma, tăng C."", ""B. Giảm gamma, giảm C (để làm 'mượt' biên giới quyết định và tăng độ rộng lề đường)."", ""C. Tăng gamma, giảm C."", ""D. Giữ nguyên gamma, tăng C.""]",B,"[""5.5_ThucHanh_SVM""]",hard
Chg5_Pra_Q10,"Câu lệnh nào sau đây tạo ra một SVM Classifier sử dụng 'Kernel Trick' đa thức bậc 3?","[""A. LinearSVC(degree=3)"", ""B. SVC(kernel='poly', degree=3)"", ""C. SVR(kernel='poly', degree=3)"", ""D. PolynomialFeatures(degree=3)""]",B,"[""5.5_ThucHanh_SVM""]",easy

Chg6_Q1_MCQ,"Tại sao Cây quyết định (Decision Tree) thường được gọi là mô hình 'Hộp trắng' (White Box)?","[""A. Vì nó luôn đưa ra dự đoán đúng."", ""B. Vì ta có thể dễ dàng hiểu, giải thích và trực quan hóa quy tắc ra quyết định của nó (theo kiểu if-else)."", ""C. Vì nó chạy nhanh."", ""D. Vì nó chỉ xử lý dữ liệu màu trắng.""]",B,"[""6.1_HuanLuyen_TrucQuan""]",easy
Chg6_Q2_MCQ,"Chỉ số 'Gini Impurity' (Độ vẩn đục Gini) của một nút bằng 0 có ý nghĩa gì?","[""A. Nút đó chứa dữ liệu thuộc về nhiều lớp khác nhau."", ""B. Nút đó là 'tinh khiết' (pure), tức là tất cả các mẫu dữ liệu trong nút đó đều thuộc về cùng một lớp."", ""C. Nút đó bị lỗi."", ""D. Nút đó chứa dữ liệu nhiễu.""]",B,"[""6.2_DuDoan_XacSuat""]",medium
Chg6_Q3_MCQ,"Sự khác biệt giữa Gini Impurity và Entropy là gì?","[""A. Gini tính toán nhanh hơn một chút, Entropy có xu hướng tạo ra cây cân bằng hơn một chút, nhưng nhìn chung kết quả thường tương tự nhau."", ""B. Gini chỉ dùng cho hồi quy, Entropy dùng cho phân loại."", ""C. Entropy luôn tốt hơn Gini."", ""D. Không có sự khác biệt nào.""]",A,"[""6.2_DuDoan_XacSuat""]",hard
Chg6_Q4_MCQ,"Thuật toán CART (Classification and Regression Tree) dùng để huấn luyện Cây quyết định hoạt động theo chiến lược nào?","[""A. Quy hoạch động (Dynamic Programming)."", ""B. Chiến lược tham lam (Greedy Algorithm): Tại mỗi bước, nó tìm cách chia tốt nhất hiện tại mà không quan tâm xem việc đó có dẫn đến kết quả tối ưu toàn cục hay không."", ""C. Gradient Descent."", ""D. Brute Force (Thử mọi trường hợp). ""]",B,"[""6.3_ThuatToan_CART""]",medium
Chg6_Q5_MCQ,"Cây quyết định có xu hướng bị Overfitting (Quá khớp) rất nặng. Tham số nào sau đây KHÔNG giúp giảm Overfitting?","[""A. Giảm max_depth (Độ sâu tối đa)."", ""B. Tăng min_samples_leaf (Số mẫu tối thiểu trong một nút lá)."", ""C. Tăng max_features (Số đặc trưng tối đa được xem xét để chia nút)."", ""D. Tăng min_samples_split.""]",C,"[""6.4_Regularization_BatOn""]",hard
Chg6_Q6_MCQ,"Độ phức tạp tính toán khi dự đoán (Prediction) của một Cây quyết định cân bằng là bao nhiêu?","[""A. O(m) - Tỷ lệ thuận với số lượng dữ liệu."", ""B. O(log(m)) - Rất nhanh, không phụ thuộc nhiều vào kích thước dữ liệu huấn luyện."", ""C. O(m^2)."", ""D. O(1). ""]",B,"[""6.3_ThuatToan_CART""]",medium
Chg6_Q7_MCQ,"Điểm yếu lớn nhất của Cây quyết định là gì?","[""A. Không thể xử lý dữ liệu số."", ""B. Rất nhạy cảm với những thay đổi nhỏ trong dữ liệu huấn luyện (Instability) và tạo ra các ranh giới quyết định vuông góc (Orthogonal)."", ""C. Khó giải thích."", ""D. Cần phải chuẩn hóa dữ liệu (Feature Scaling) trước khi dùng.""]",B,"[""6.4_Regularization_BatOn""]",medium
Chg6_Q8_MCQ,"Trong bài toán Hồi quy (Decision Tree Regression), giá trị dự đoán cho một mẫu mới là gì?","[""A. Lớp phổ biến nhất trong nút lá."", ""B. Giá trị trung bình (Average) của các mẫu huấn luyện nằm trong nút lá tương ứng."", ""C. Giá trị ngẫu nhiên."", ""D. Giá trị lớn nhất trong nút lá.""]",B,"[""6.3_ThuatToan_CART""]",medium
Chg6_Q9_MCQ,"Phương pháp PCA (Phân tích thành phần chính) thường được dùng kết hợp với Cây quyết định để làm gì?","[""A. Để tăng số chiều dữ liệu."", ""B. Để xoay trục dữ liệu, giúp Cây quyết định tìm ra các ranh giới phân chia tốt hơn (vì Cây chỉ cắt được đường vuông góc với trục)."", ""C. Để làm chậm quá trình huấn luyện."", ""D. Để chuyển bài toán hồi quy thành phân loại.""]",B,"[""6.4_Regularization_BatOn""]",hard

Chg6_Pra_Q3,"Hàm `export_graphviz` trong Scikit-Learn xuất ra định dạng file nào để vẽ cây quyết định?","[""A. .png (ảnh)"", ""B. .dot (định dạng văn bản mô tả đồ thị)"", ""C. .xml"", ""D. .html""]",B,"[""6.5_ThucHanh_CayQuyetDinh""]",easy
Chg6_Pra_Q4,"Trong `DecisionTreeClassifier`, phương thức `predict_proba(X)` trả về giá trị như thế nào cho một mẫu dữ liệu?","[""A. Trả về 0 hoặc 1."", ""B. Trả về tỷ lệ phần trăm các mẫu thuộc lớp đó trong nút lá (leaf node) mà mẫu dữ liệu rơi vào (ví dụ: 0.9 cho lớp A, 0.1 cho lớp B)."", ""C. Trả về khoảng cách đến siêu phẳng."", ""D. Trả về độ sâu của nút lá.""]",B,"[""6.5_ThucHanh_CayQuyetDinh""]",medium
Chg6_Pra_Q5,"Khi sử dụng `DecisionTreeRegressor`, nếu bạn thiết lập `max_depth=2`, mô hình sẽ trông như thế nào trên biểu đồ 2D?","[""A. Một đường thẳng chéo (như Linear Regression)."", ""B. Một đường cong trơn (như Polynomial Regression)."", ""C. Một hàm bậc thang (step function) chia không gian thành các vùng hình chữ nhật và dự đoán giá trị hằng số trong mỗi vùng."", ""D. Một hình tròn.""]",C,"[""6.5_ThucHanh_CayQuyetDinh""]",hard
Chg6_Pra_Q6,"Để giảm Overfitting cho Cây quyết định, bạn nên thay đổi tham số nào sau đây?","[""A. Tăng `max_depth`."", ""B. Giảm `min_samples_leaf`."", ""C. Tăng `min_samples_leaf` (yêu cầu mỗi nút lá phải có nhiều mẫu hơn, tránh tạo ra các nhánh quá nhỏ chỉ khớp với nhiễu)."", ""D. Tăng `max_features` lên tối đa.""]",C,"[""6.5_ThucHanh_CayQuyetDinh""]",medium
Chg6_Pra_Q7,"Thuộc tính `tree_.feature_importances_` (hoặc `clf.feature_importances_`) được tính toán dựa trên tiêu chí nào?","[""A. Số lần đặc trưng xuất hiện trong cây."", ""B. Tổng độ giảm độ vẩn đục (Gini impurity reduction) có trọng số mà đặc trưng đó mang lại cho toàn bộ cây."", ""C. Hệ số tương quan Pearson."", ""D. Giá trị trung bình của đặc trưng.""]",B,"[""6.5_ThucHanh_CayQuyetDinh""]",hard
Chg6_Pra_Q8,"Mặc định, `DecisionTreeClassifier` sử dụng tiêu chí nào để chia nút (`criterion`)?","[""A. criterion='gini'"", ""B. criterion='entropy'"", ""C. criterion='log_loss'"", ""D. criterion='mae' ""]",A,"[""6.5_ThucHanh_CayQuyetDinh""]",easy
Chg6_Pra_Q9,"Cây quyết định trong Scikit-Learn có hỗ trợ các biến phân loại (categorical features) dạng chuỗi văn bản trực tiếp không?","[""A. Có, nó tự động xử lý."", ""B. Không, bạn bắt buộc phải mã hóa chúng thành số (ví dụ dùng `OrdinalEncoder` hoặc `OneHotEncoder`) trước khi đưa vào huấn luyện."", ""C. Có, nhưng chỉ với tiếng Anh."", ""D. Tùy thuộc vào phiên bản.""]",B,"[""6.5_ThucHanh_CayQuyetDinh""]",medium
Chg6_Pra_Q10,"Nếu bạn đặt `min_samples_split=10` và một nút chỉ có 5 mẫu dữ liệu, điều gì sẽ xảy ra với nút đó?","[""A. Nút đó sẽ tiếp tục được chia nhỏ."", ""B. Nút đó sẽ trở thành nút lá (leaf node) và không bị chia nữa, bất kể độ vẩn đục (impurity) là bao nhiêu."", ""C. Nút đó bị xóa."", ""D. Nút đó sẽ báo lỗi.""]",B,"[""6.5_ThucHanh_CayQuyetDinh""]",medium

Chg7_Q1_MCQ,"Nguyên lý 'Sức mạnh của đám đông' (Wisdom of the Crowd) trong Ensemble Learning thể hiện điều gì?","[""A. Một nhóm các mô hình dự đoán yếu (tốt hơn ngẫu nhiên một chút) khi kết hợp lại có thể tạo thành một mô hình mạnh, miễn là chúng đủ đa dạng."", ""B. Dùng càng nhiều dữ liệu càng tốt."", ""C. Mô hình phức tạp nhất luôn là mô hình tốt nhất."", ""D. Chỉ nên dùng một thuật toán duy nhất.""]",A,"[""7.1_BauChon_SoDong""]",easy
Chg7_Q2_MCQ,"Sự khác biệt giữa 'Hard Voting' và 'Soft Voting' trong các bộ phân loại là gì?","[""A. Hard Voting tính trung bình xác suất, Soft Voting lấy theo đa số phiếu bầu."", ""B. Hard Voting lấy theo đa số phiếu bầu (lớp được chọn nhiều nhất). Soft Voting tính trung bình xác suất (probabilities) của tất cả các mô hình và chọn lớp có xác suất cao nhất."", ""C. Hard Voting dùng cho phần cứng, Soft Voting dùng cho phần mềm."", ""D. Soft Voting luôn kém chính xác hơn Hard Voting.""]",B,"[""7.1_BauChon_SoDong""]",medium
Chg7_Q3_MCQ,"Sự khác biệt chính giữa kỹ thuật 'Bagging' và 'Pasting' là gì?","[""A. Bagging lấy mẫu CÓ hoàn lại (replacement) - một mẫu có thể được chọn nhiều lần. Pasting lấy mẫu KHÔNG hoàn lại."", ""B. Bagging dùng cho dữ liệu ảnh, Pasting dùng cho văn bản."", ""C. Pasting cho phép huấn luyện song song, Bagging thì không."", ""D. Bagging là kỹ thuật cũ, Pasting mới hơn.""]",A,"[""7.2_Bagging_Pasting""]",medium
Chg7_Q4_MCQ,"Đánh giá OOB (Out-of-Bag Evaluation) trong Bagging có tác dụng gì?","[""A. Nó thay thế tập kiểm tra (Test Set) bằng cách sử dụng chính những mẫu dữ liệu không được chọn trong quá trình lấy mẫu (khoảng 37%) để đánh giá mô hình."", ""B. Nó giúp tăng tốc độ huấn luyện."", ""C. Nó dùng để loại bỏ các đặc trưng thừa."", ""D. Nó là một phương pháp tinh chỉnh siêu tham số.""]",A,"[""7.2_Bagging_Pasting""]",hard
Chg7_Q5_MCQ,"Random Forest (Rừng ngẫu nhiên) khác với Bagging Decision Tree thông thường ở điểm nào?","[""A. Random Forest sử dụng SVM thay vì Cây quyết định."", ""B. Random Forest thêm một lớp ngẫu nhiên nữa: Tại mỗi nút chia, nó chỉ tìm kiếm đặc trưng tốt nhất trong một tập hợp con ngẫu nhiên các đặc trưng (chứ không phải tất cả đặc trưng)."", ""C. Random Forest không dùng kỹ thuật Bootstrap."", ""D. Random Forest dễ bị Overfitting hơn.""]",B,"[""7.3_Random_Forests""]",medium
Chg7_Q6_MCQ,"Tại sao Random Forest lại giúp giảm Overfitting tốt hơn so với một Cây quyết định đơn lẻ?","[""A. Vì nó tính toán kỹ lưỡng hơn."", ""B. Vì bằng cách trung bình hóa kết quả của nhiều cây (vốn có độ chệch thấp nhưng phương sai cao), nó giúp giảm phương sai (Variance) của mô hình tổng thể."", ""C. Vì nó sử dụng ít dữ liệu hơn."", ""D. Vì nó cắt tỉa cây chặt chẽ hơn.""]",B,"[""7.3_Random_Forests""]",medium
Chg7_Q7_MCQ,"Thuật toán AdaBoost hoạt động theo cơ chế nào?","[""A. Huấn luyện các mô hình song song độc lập."", ""B. Huấn luyện tuần tự: Mô hình sau cố gắng sửa sai cho mô hình trước bằng cách gán trọng số cao hơn cho các mẫu dữ liệu bị phân loại sai."", ""C. Huấn luyện trên phần dư (Residuals) của mô hình trước."", ""D. Sử dụng mạng nơ-ron.""]",B,"[""7.4_Boosting""]",medium
Chg7_Q8_MCQ,"Gradient Boosting khác AdaBoost ở điểm nào?","[""A. Gradient Boosting không thay đổi trọng số của dữ liệu, mà huấn luyện mô hình mới để dự đoán các 'sai số còn dư' (residual errors) của mô hình trước đó."", ""B. Gradient Boosting yếu hơn AdaBoost."", ""C. Gradient Boosting chỉ dùng được cho bài toán hồi quy."", ""D. Gradient Boosting không thể bị Overfitting.""]",A,"[""7.4_Boosting""]",hard
Chg7_Q9_MCQ,"Kỹ thuật 'Stacking' (Stacked Generalization) làm gì thay vì chỉ bình chọn đơn giản?","[""A. Nó loại bỏ các mô hình yếu."", ""B. Nó huấn luyện một mô hình 'meta-learner' (bộ trộn) để học cách kết hợp các dự đoán từ các mô hình cơ sở một cách tối ưu nhất."", ""C. Nó xếp chồng dữ liệu lên nhau."", ""D. Nó chạy tất cả mô hình cùng lúc.""]",B,"[""7.5_Stacking""]",hard
Chg7_Q10_MCQ,"Tính năng 'Feature Importance' (Tầm quan trọng đặc trưng) trong Random Forest được tính dựa trên điều gì?","[""A. Số lần đặc trưng đó xuất hiện trong dữ liệu."", ""B. Mức độ mà đặc trưng đó giúp giảm độ vẩn đục (Impurity) trung bình trên tất cả các cây trong rừng."", ""C. Tương quan tuyến tính với nhãn."", ""D. Do người dùng tự định nghĩa.""]",B,"[""7.3_Random_Forests""]",medium

Chg7_Pra_Q3,"Để khởi tạo một `VotingClassifier` với chế độ 'Soft Voting' (dựa trên xác suất trung bình), điều kiện tiên quyết đối với các bộ phân loại thành phần (estimators) là gì?","[""A. Chúng phải cùng là cây quyết định."", ""B. Chúng phải hỗ trợ phương thức `predict_proba()`. Với SVC, bạn phải đặt `probability=True`."", ""C. Chúng phải có cùng tốc độ học."", ""D. Chúng phải được huấn luyện trên các tập dữ liệu khác nhau.""]",B,"[""7.6_ThucHanh_Ensemble""]",medium
Chg7_Pra_Q4,"Khi sử dụng `BaggingClassifier`, để kích hoạt tính năng đánh giá trên các mẫu không được chọn (Out-of-Bag evaluation) và xem kết quả, bạn cần làm gì?","[""A. Đặt `oob_score=True` khi khởi tạo và gọi thuộc tính `clf.oob_score_` sau khi huấn luyện."", ""B. Gọi hàm `clf.evaluate_oob()`."", ""C. Đặt `bootstrap=False`."", ""D. Dùng `cross_val_score`.""]",A,"[""7.6_ThucHanh_Ensemble""]",medium
Chg7_Pra_Q5,"Trong `RandomForestClassifier`, thuộc tính `feature_importances_` trả về một mảng giá trị có tổng bằng bao nhiêu?","[""A. Bằng 1.0 (được chuẩn hóa)."", ""B. Bằng số lượng đặc trưng."", ""C. Bằng số lượng cây."", ""D. Không giới hạn.""]",A,"[""7.6_ThucHanh_Ensemble""]",easy
Chg7_Pra_Q6,"Sự khác biệt về mặt code giữa `RandomForestClassifier` và `ExtraTreesClassifier` chủ yếu nằm ở đâu?","[""A. `ExtraTreesClassifier` nhanh hơn vì nó chọn ngưỡng phân chia (threshold) ngẫu nhiên cho mỗi đặc trưng thay vì tìm kiếm ngưỡng tối ưu như Random Forest."", ""B. `ExtraTreesClassifier` dùng ít cây hơn."", ""C. `ExtraTreesClassifier` chỉ dùng cho hồi quy."", ""D. Không có sự khác biệt.""]",A,"[""7.6_ThucHanh_Ensemble""]",hard
Chg7_Pra_Q7,"Để cài đặt thuật toán AdaBoost sử dụng xác suất lớp (Class Probabilities) thay vì nhãn lớp (giúp hội tụ nhanh hơn), Scikit-Learn sử dụng thuật toán nào mặc định cho `AdaBoostClassifier`?","[""A. SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss)."", ""B. SAMME.R (R nghĩa là Real - số thực)."", ""C. Gradient Boosting."", ""D. LogitBoost.""]",B,"[""7.6_ThucHanh_Ensemble""]",hard
Chg7_Pra_Q8,"Khi sử dụng `GradientBoostingRegressor`, để thực hiện 'Early Stopping' (dừng sớm) nhằm tìm số lượng cây tối ưu mà không cần huấn luyện lại từ đầu, ta có thể dùng phương thức nào để lấy lỗi trên từng giai đoạn?","[""A. gbrt.staged_predict(X_val)"", ""B. gbrt.predict_all(X_val)"", ""C. gbrt.errors_"", ""D. gbrt.history_""]",A,"[""7.6_ThucHanh_Ensemble""]",hard
Chg7_Pra_Q9,"Thư viện `XGBoost` (Extreme Gradient Boosting) cung cấp class nào tương thích với giao diện Scikit-Learn (fit/predict)?","[""A. `xgboost.XGBClassifier` và `xgboost.XGBRegressor`"", ""B. `xgboost.Tree()`"", ""C. `xgboost.Ensemble()`"", ""D. `xgboost.Model()`""]",A,"[""7.6_ThucHanh_Ensemble""]",easy
Chg7_Pra_Q10,"Tham số `warm_start=True` trong `RandomForestClassifier` hoặc `GradientBoostingClassifier` cho phép làm gì?","[""A. Giữ lại các cây đã huấn luyện và thêm cây mới vào mô hình khi gọi `.fit()` lần nữa (hữu ích cho việc tăng dần số lượng cây)."", ""B. Làm nóng CPU trước khi chạy."", ""C. Tự động điều chỉnh tốc độ học."", ""D. Xóa bỏ các cây cũ.""]",A,"[""7.6_ThucHanh_Ensemble""]",medium

Chg8_Q1_MCQ,"'Lời nguyền số chiều' (Curse of Dimensionality) gây ra vấn đề gì nghiêm trọng nhất?","[""A. Dữ liệu trở nên rất thưa thớt (sparse) trong không gian cao chiều, khiến khoảng cách giữa các điểm mất đi ý nghĩa và nguy cơ Overfitting tăng cao."", ""B. Máy tính không đủ bộ nhớ RAM."", ""C. Dữ liệu bị trùng lặp quá nhiều."", ""D. Tốc độ huấn luyện quá nhanh.""]",A,"[""8.1_LoiNguyen_SoChieu""]",medium
Chg8_Q2_MCQ,"Hai hướng tiếp cận chính để giảm chiều dữ liệu là gì?","[""A. Phép chiếu (Projection) và Học đa tạp (Manifold Learning)."", ""B. PCA và SVM."", ""C. Tuyến tính và Phi tuyến."", ""D. Cắt bỏ và Giữ lại.""]",A,"[""8.2_Chieu_Manifold""]",medium
Chg8_Q3_MCQ,"Thuật toán PCA (Principal Component Analysis) hoạt động dựa trên nguyên tắc nào?","[""A. Tìm siêu phẳng phân chia các lớp tốt nhất."", ""B. Tìm trục siêu phẳng mà dữ liệu có phương sai (variance) lớn nhất để chiếu dữ liệu lên đó (tức là giữ lại nhiều thông tin nhất)."", ""C. Tìm các điểm trung tâm của cụm."", ""D. Xóa bỏ các đặc trưng có giá trị nhỏ.""]",B,"[""8.3_PCA""]",easy
Chg8_Q4_MCQ,"'Tỷ lệ phương sai giải thích' (Explained Variance Ratio) trong PCA cho ta biết điều gì?","[""A. Tỷ lệ dữ liệu bị mất đi."", ""B. Phần trăm thông tin (phương sai) của tập dữ liệu ban đầu nằm trên mỗi trục thành phần chính (Principal Component)."", ""C. Độ chính xác của mô hình."", ""D. Số lượng đặc trưng còn lại.""]",B,"[""8.3_PCA""]",medium
Chg8_Q5_MCQ,"Khi nào thì phương pháp 'Phép chiếu' (Projection) như PCA hoạt động kém hiệu quả?","[""A. Khi dữ liệu nằm trên một mặt phẳng phẳng."", ""B. Khi dữ liệu bị cuộn tròn xoắn ốc (ví dụ: Swiss Roll dataset), việc chiếu thẳng xuống sẽ làm đè nén và mất cấu trúc dữ liệu."", ""C. Khi dữ liệu có quá nhiều dòng."", ""D. Khi dữ liệu là ảnh đen trắng.""]",B,"[""8.2_Chieu_Manifold""]",hard
Chg8_Q6_MCQ,"Kernel PCA (kPCA) là phiên bản mở rộng của PCA để giải quyết vấn đề gì?","[""A. Giảm chiều dữ liệu phi tuyến (Non-linear) bằng cách sử dụng Kernel Trick (tương tự SVM)."", ""B. Tăng tốc độ tính toán cho PCA."", ""C. Dùng cho dữ liệu văn bản."", ""D. Loại bỏ nhiễu tốt hơn PCA thường.""]",A,"[""8.4_Kernel_PCA""]",medium
Chg8_Q7_MCQ,"Thuật toán LLE (Locally Linear Embedding) thuộc nhóm Manifold Learning hoạt động như thế nào?","[""A. Nó cố gắng bảo toàn khoảng cách toàn cục (Global distance) giữa mọi cặp điểm."", ""B. Nó cố gắng bảo toàn mối quan hệ lân cận địa phương (Local relationships): Nếu hai điểm gần nhau trong không gian gốc, chúng cũng phải gần nhau trong không gian mới."", ""C. Nó sử dụng mạng nơ-ron."", ""D. Nó chiếu dữ liệu ngẫu nhiên.""]",B,"[""8.5_LLE_Isomap""]",hard
Chg8_Q8_MCQ,"Một ứng dụng quan trọng khác của PCA ngoài việc giảm chiều dữ liệu là gì?","[""A. Tăng độ chính xác của Random Forest."", ""B. Nén dữ liệu (Data Compression) và Khử nhiễu (Noise Filtering) bằng cách loại bỏ các thành phần có phương sai thấp."", ""C. Dự báo thời tiết."", ""D. Phân loại ảnh màu.""]",B,"[""8.3_PCA""]",medium
Chg8_Q9_MCQ,"Tại sao ta thường chọn số lượng chiều (số thành phần chính) sao cho tổng phương sai giải thích đạt khoảng 95%?","[""A. Để đảm bảo giữ lại hầu hết thông tin quan trọng của dữ liệu gốc mà vẫn giảm được kích thước đáng kể."", ""B. Vì con số 95% là bắt buộc trong mọi thuật toán."", ""C. Để làm tròn số."", ""D. Để loại bỏ 95% dữ liệu.""]",A,"[""8.3_PCA""]",easy
Chg8_Q10_MCQ,"Randomized PCA hữu ích trong trường hợp nào?","[""A. Khi cần độ chính xác tuyệt đối."", ""B. Khi bộ dữ liệu rất lớn và ta muốn tìm gần đúng d thành phần chính đầu tiên cực nhanh thay vì dùng SVD đầy đủ (chậm)."", ""C. Khi dữ liệu quá nhỏ."", ""D. Khi muốn tạo dữ liệu ngẫu nhiên.""]",B,"[""8.3_PCA""]",hard

Chg8_Pra_Q3,"Sau khi khớp `pca = PCA(n_components=2)` với dữ liệu, thuộc tính nào chứa các thành phần chính (các vector đơn vị định nghĩa trục mới)?","[""A. pca.components_"", ""B. pca.explained_variance_"", ""C. pca.vectors_"", ""D. pca.axes_""]",A,"[""8.6_ThucHanh_GiamChieu""]",medium
Chg8_Pra_Q4,"Khi bộ dữ liệu quá lớn không thể chứa hết trong bộ nhớ (RAM) để chạy PCA tiêu chuẩn, bạn nên dùng class nào của Scikit-Learn để huấn luyện theo từng lô (mini-batch)?","[""A. `IncrementalPCA` (sử dụng phương thức `partial_fit`)`."", ""B. `RandomizedPCA`."", ""C. `KernelPCA`."", ""D. `SparsePCA`.""]",A,"[""8.6_ThucHanh_GiamChieu""]",medium
Chg8_Pra_Q5,"Để tối ưu hóa các siêu tham số của `KernelPCA` (như `gamma`, `kernel`) trong một Pipeline cho bài toán phân loại, bước cuối cùng của Pipeline thường là gì?","[""A. Một bộ phân loại (ví dụ: `LogisticRegression`). Ta dùng GridSearch trên toàn bộ Pipeline để tìm bộ tham số KernelPCA giúp bộ phân loại đạt độ chính xác cao nhất."", ""B. Không cần bước cuối cùng."", ""C. `InverseTransform`."", ""D. `StandardScaler`.""]",A,"[""8.6_ThucHanh_GiamChieu""]",hard
Chg8_Pra_Q6,"Trong `KernelPCA`, làm thế nào để tính toán lỗi tái tạo (reconstruction error) của một điểm dữ liệu (pre-image)?","[""A. Gọi `kpca.reconstruction_error(X)` trực tiếp."", ""B. Phải đặt `fit_inverse_transform=True` khi khởi tạo, sau đó gọi `inverse_transform` để tái tạo và tự tính khoảng cách MSE giữa ảnh gốc và ảnh tái tạo."", ""C. Kernel PCA không thể tái tạo dữ liệu."", ""D. Dùng `kpca.score(X)`.""]",B,"[""8.6_ThucHanh_GiamChieu""]",hard
Chg8_Pra_Q7,"Để giảm chiều dữ liệu nhằm trực quan hóa (ví dụ: từ hàng trăm chiều xuống 2D/3D) và phân tách các cụm dữ liệu tốt nhất (như MNIST), thuật toán nào thường được ưu tiên hơn PCA dù chạy chậm hơn?","[""A. t-SNE (t-Distributed Stochastic Neighbor Embedding)."", ""B. LLE (Locally Linear Embedding)."", ""C. Isomap."", ""D. MDS (Multidimensional Scaling).""]",A,"[""8.6_ThucHanh_GiamChieu""]",easy
Chg8_Pra_Q8,"Khi sử dụng `PCA` với `svd_solver='randomized'`, tham số nào kiểm soát tốc độ và độ chính xác của thuật toán ngẫu nhiên này?","[""A. `iterated_power` (số lần lặp) và `n_components`."", ""B. `learning_rate`."", ""C. `epsilon`."", ""D. `batch_size`.""]",A,"[""8.6_ThucHanh_GiamChieu""]",hard
Chg8_Pra_Q9,"Lớp `LocallyLinearEmbedding` (LLE) có tham số `n_neighbors`. Nếu `n_neighbors` quá nhỏ, chuyện gì xảy ra?","[""A. Mô hình sẽ học cấu trúc cục bộ quá chi tiết (dễ bị nhiễu/Overfitting với cấu trúc địa phương) và mất đi cái nhìn toàn cảnh."", ""B. Mô hình sẽ chạy rất chậm."", ""C. Mô hình sẽ biến thành PCA."", ""D. Mô hình không hoạt động.""]",A,"[""8.6_ThucHanh_GiamChieu""]",medium
Chg8_Pra_Q10,"Trong Scikit-Learn, phương thức `fit_transform()` của PCA thực hiện hai bước gì?","[""A. Tìm các trục chính (fit) và chiếu dữ liệu lên các trục đó (transform)."", ""B. Chuẩn hóa dữ liệu và xóa dữ liệu."", ""C. Huấn luyện mô hình và dự báo."", ""D. Xóa các cột trùng lặp.""]",A,"[""8.6_ThucHanh_GiamChieu""]",easy

Chg9_Q1_MCQ,"Mục tiêu toán học của thuật toán phân cụm K-Means là gì?","[""A. Tối thiểu hóa khoảng cách giữa các cụm."", ""B. Tối thiểu hóa 'Inertia' (Tổng bình phương khoảng cách từ các điểm dữ liệu đến tâm cụm gần nhất) và tối đa hóa khoảng cách giữa các tâm cụm."", ""C. Tìm các điểm nhiễu."", ""D. Xây dựng cây phân cấp.""]",B,"[""9.1_K_Means""]",medium
Chg9_Q2_MCQ,"Nhược điểm lớn nhất của thuật toán K-Means khiến nó thất bại trong nhiều trường hợp thực tế là gì?","[""A. Chạy rất chậm."", ""B. Phải biết trước số lượng cụm (K) và thuật toán hoạt động rất kém khi các cụm có hình dạng không tròn (non-spherical), kích thước chênh lệch hoặc mật độ khác nhau."", ""C. Không xử lý được dữ liệu số."", ""D. Luôn tìm được cực tiểu toàn cục.""]",B,"[""9.1_K_Means""]",medium
Chg9_Q3_MCQ,"Phương pháp 'Khuỷu tay' (Elbow Method) giúp ta làm gì trong K-Means?","[""A. Tìm vị trí tâm cụm tốt nhất."", ""B. Chọn số lượng cụm (K) tối ưu bằng cách tìm điểm gập khúc trên đồ thị Inertia, tại đó việc tăng thêm cụm không làm giảm lỗi đáng kể nữa."", ""C. Loại bỏ nhiễu."", ""D. Tăng tốc độ hội tụ.""]",B,"[""9.1_K_Means""]",easy
Chg9_Q4_MCQ,"Hệ số dáng điệu (Silhouette Score) của một điểm dữ liệu gần bằng +1 có ý nghĩa gì?","[""A. Điểm đó nằm sai cụm."", ""B. Điểm đó nằm gần biên giới giữa hai cụm (nhập nhằng)."", ""C. Điểm đó nằm gọn gàng trong cụm của nó và cách xa các cụm khác (Phân cụm tốt)."", ""D. Điểm đó là nhiễu.""]",C,"[""9.1_K_Means""]",hard
Chg9_Q5_MCQ,"Thuật toán DBSCAN phân cụm dựa trên yếu tố nào?","[""A. Khoảng cách đến tâm cụm."", ""B. Mật độ (Density): Nó nhóm các điểm nằm trong vùng có mật độ cao liên tục và coi các điểm nằm ở vùng thưa thớt là nhiễu (noise)."", ""C. Phân phối xác suất Gaussian."", ""D. Cây quyết định.""]",B,"[""9.2_DBSCAN""]",easy
Chg9_Q6_MCQ,"Ưu điểm vượt trội của DBSCAN so với K-Means là gì?","[""A. Không cần biết trước số lượng cụm và có thể phát hiện các cụm có hình dạng bất kỳ (cong, xoắn ốc...) cũng như tự động loại bỏ nhiễu."", ""B. Chạy nhanh hơn trên mọi dữ liệu."", ""C. Luôn gán mọi điểm vào một cụm nào đó."", ""D. Dễ cài đặt hơn.""]",A,"[""9.2_DBSCAN""]",medium
Chg9_Q7_MCQ,"Trong DBSCAN, hai tham số quan trọng nhất là Epsilon (ε) và min_samples dùng để định nghĩa điều gì?","[""A. Số lượng cụm."", ""B. 'Vùng lân cận' và 'Độ đặc' tối thiểu để xác định một điểm lõi (Core Point)."", ""C. Tốc độ học."", ""D. Số lần lặp lại.""]",B,"[""9.2_DBSCAN""]",hard
Chg9_Q8_MCQ,"Mô hình Hỗn hợp Gaussian (Gaussian Mixture Model - GMM) thực hiện phân cụm theo cơ chế nào?","[""A. Hard Clustering (Phân cụm cứng)."", ""B. Soft Clustering (Phân cụm mềm): Mỗi điểm có một xác suất thuộc về từng cụm Gaussian (hình elip) khác nhau."", ""C. Hierarchical Clustering."", ""D. Grid-based Clustering.""]",B,"[""9.3_Gaussian_Mixtures""]",medium
Chg9_Q9_MCQ,"Thuật toán EM (Expectation-Maximization) được dùng trong GMM có mục đích gì?","[""A. Để tìm các tham số (trung bình, phương sai, tỷ trọng) của các phân phối Gaussian sao cho khớp với dữ liệu nhất."", ""B. Để đếm số lượng điểm dữ liệu."", ""C. Để vẽ biểu đồ."", ""D. Để tính đạo hàm.""]",A,"[""9.3_Gaussian_Mixtures""]",hard
Chg9_Q10_MCQ,"Ngoài phân cụm, GMM thường được ứng dụng hiệu quả cho bài toán nào?","[""A. Phân loại ảnh."", ""B. Phát hiện bất thường (Anomaly Detection): Những điểm nằm ở vùng có mật độ xác suất quá thấp được coi là bất thường."", ""C. Hồi quy tuyến tính."", ""D. Dự báo thời tiết.""]",B,"[""9.3_Gaussian_Mixtures""]",medium

Chg9_Pra_Q3,"Để tránh rơi vào cực tiểu địa phương (local optima) khi chạy thuật toán K-Means, tham số `n_init` dùng để làm gì?","[""A. Quy định số lượng tâm cụm."", ""B. Quy định số lần chạy lại thuật toán với các khởi tạo tâm ngẫu nhiên khác nhau (Scikit-Learn sẽ giữ lại kết quả có Inertia thấp nhất)."", ""C. Quy định số vòng lặp tối đa."", ""D. Quy định số luồng CPU.""]",B,"[""9.4_ThucHanh_Unsupervised""]",medium
Chg9_Pra_Q4,"Hàm `silhouette_score(X, labels)` trong module `sklearn.metrics` yêu cầu đầu vào là gì?","[""A. Dữ liệu gốc X và nhãn cụm dự đoán."", ""B. Chỉ cần nhãn cụm."", ""C. Dữ liệu X và tâm cụm."", ""D. Inertia.""]",A,"[""9.4_ThucHanh_Unsupervised""]",easy
Chg9_Pra_Q5,"Khi sử dụng `GaussianMixture`, tham số `covariance_type='tied'` có ý nghĩa gì?","[""A. Tất cả các cụm đều phải có hình cầu (spherical)."", ""B. Tất cả các cụm chia sẻ cùng một ma trận hiệp phương sai chung (có cùng hình dạng, kích thước và hướng), giúp giảm số lượng tham số cần học."", ""C. Mỗi cụm có ma trận hiệp phương sai riêng biệt (full)."", ""D. Ma trận hiệp phương sai là đường chéo.""]",B,"[""9.4_ThucHanh_Unsupervised""]",hard
Chg9_Pra_Q6,"Để chọn số lượng thành phần (số cụm) tối ưu cho mô hình Gaussian Mixture, ta thường so sánh hai chỉ số thông tin nào (càng thấp càng tốt)?","[""A. RMSE và MAE."", ""B. AIC (Akaike Information Criterion) và BIC (Bayesian Information Criterion)."", ""C. Inertia và Silhouette score."", ""D. Accuracy và F1.""]",B,"[""9.4_ThucHanh_Unsupervised""]",medium
Chg9_Pra_Q7,"Thuật toán DBSCAN trong Scikit-Learn có phương thức `.predict()` để dự báo cụm cho dữ liệu mới không?","[""A. Có, nó hoạt động giống K-Means."", ""B. Không, DBSCAN không hỗ trợ dự báo cho dữ liệu mới (transductive). Bạn phải dùng một thuật toán phân loại khác (ví dụ KNeighborsClassifier) huấn luyện trên các điểm lõi (core samples) của DBSCAN để dự đoán."", ""C. Có, nhưng rất chậm."", ""D. Tùy thuộc vào phiên bản.""]",B,"[""9.4_ThucHanh_Unsupervised""]",hard
Chg9_Pra_Q8,"Để sử dụng Gaussian Mixture cho bài toán 'Phát hiện bất thường' (Anomaly Detection), ta dùng phương thức nào để đánh giá mật độ xác suất tại mỗi điểm dữ liệu?","[""A. `gm.predict(X)`"", ""B. `gm.score_samples(X)` (trả về log của hàm mật độ xác suất - log-likelihood; giá trị càng thấp nghĩa là điểm đó càng bất thường)."", ""C. `gm.predict_proba(X)`"", ""D. `gm.transform(X)`""]",B,"[""9.4_ThucHanh_Unsupervised""]",hard
Chg9_Pra_Q9,"Trong bài toán phân đoạn hình ảnh (Image Segmentation) bằng K-Means, tại sao ta phải dùng lệnh `image.reshape(-1, 3)` trước khi đưa vào mô hình?","[""A. Để chuyển ảnh thành đen trắng."", ""B. Để làm phẳng mảng ảnh 2D (height, width, colors) thành một danh sách dài các pixel (mỗi pixel là một vector màu 3 chiều), phù hợp với đầu vào của K-Means."", ""C. Để giảm kích thước ảnh."", ""D. Để tăng độ sáng.""]",B,"[""9.4_ThucHanh_Unsupervised""]",medium
Chg9_Pra_Q10,"Trong K-Means, sau khi huấn luyện, làm thế nào để lấy tọa độ của các tâm cụm?","[""A. `kmeans.labels_`"", ""B. `kmeans.cluster_centers_`"", ""C. `kmeans.centroids_`"", ""D. `kmeans.means_`""]",B,"[""9.4_ThucHanh_Unsupervised""]",easy

Chg10_Q1_MCQ,"Perceptron (Mạng nơ-ron đơn giản nhất) có hạn chế lớn nào khiến nó từng bị 'ghẻ lạnh' trong lịch sử AI (liên quan đến bài toán XOR)?","[""A. Nó không thể giải quyết các bài toán không phân tách tuyến tính (Non-linearly separable)."", ""B. Nó tính toán quá chậm."", ""C. Nó cần quá nhiều dữ liệu."", ""D. Nó không có trọng số.""]",A,"[""10.1_Perceptron_LyThuyet""]",medium
Chg10_Q2_MCQ,"Thuật toán 'Lan truyền ngược' (Backpropagation) hoạt động theo quy trình nào?","[""A. Chỉ chạy xuôi (Forward) để dự đoán."", ""B. Dự đoán (Forward pass) -> Tính sai số -> Truyền ngược sai số (Reverse pass) để đo đóng góp của từng kết nối -> Cập nhật trọng số (Gradient Descent Step)."", ""C. Chọn ngẫu nhiên trọng số cho đến khi đúng."", ""D. Xây dựng cây quyết định.""]",B,"[""10.2_MLP_Backprop""]",hard
Chg10_Q3_MCQ,"Tại sao hàm kích hoạt phi tuyến (Non-linear Activation Function) như ReLU, Sigmoid lại bắt buộc phải có trong các lớp ẩn của MLP?","[""A. Để làm cho đầu ra đẹp hơn."", ""B. Nếu không có phi tuyến, chồng nhiều lớp lên nhau (dù sâu đến đâu) cũng chỉ tương đương với một lớp tuyến tính duy nhất, không thể giải quyết bài toán phức tạp."", ""C. Để đưa giá trị về 0."", ""D. Để tăng tốc độ tính toán.""]",B,"[""10.2_MLP_Backprop""]",medium
Chg10_Q4_MCQ,"Trong API Sequential của Keras, phương thức 'model.compile()' dùng để làm gì?","[""A. Bắt đầu quá trình huấn luyện."", ""B. Định nghĩa hàm mất mát (Loss), thuật toán tối ưu (Optimizer) và các chỉ số đánh giá (Metrics)."", ""C. Thêm các lớp vào mô hình."", ""D. Dự đoán kết quả.""]",B,"[""10.3_Keras_Sequential""]",easy
Chg10_Q5_MCQ,"Khi xây dựng mô hình phân loại đa lớp (Multiclass Classification) với các nhãn là số nguyên (0, 1, 2...), bạn nên dùng hàm mất mát (Loss function) nào trong Keras?","[""A. 'binary_crossentropy'."", ""B. 'categorical_crossentropy' (yêu cầu one-hot encoding)."", ""C. 'sparse_categorical_crossentropy' (dùng trực tiếp số nguyên)."", ""D. 'mean_squared_error'.""]",C,"[""10.3_Keras_Sequential""]",hard
Chg10_Q6_MCQ,"Hàm kích hoạt ReLU (Rectified Linear Unit) thường được ưu tiên sử dụng cho các lớp ẩn vì lý do gì?","[""A. Nó luôn cho kết quả âm."", ""B. Nó đơn giản, tính toán nhanh và giảm thiểu vấn đề 'Biến mất đạo hàm' (Vanishing Gradient) tốt hơn Sigmoid/Tanh."", ""C. Nó giới hạn đầu ra trong khoảng [0, 1]."", ""D. Nó là hàm tuyến tính.""]",B,"[""10.2_MLP_Backprop""]",medium
Chg10_Q7_MCQ,"Hàm kích hoạt nào thường được sử dụng ở lớp đầu ra (Output Layer) cho bài toán phân loại nhị phân (Binary Classification)?","[""A. Softmax."", ""B. Sigmoid (đưa giá trị về khoảng 0-1 xác suất)."", ""C. ReLU."", ""D. Linear.""]",B,"[""10.3_Keras_Sequential""]",easy
Chg10_Q8_MCQ,"Khi nào bạn bắt buộc phải dùng Keras Functional API thay vì Sequential API?","[""A. Khi mô hình chỉ là một chồng các lớp đơn giản xếp lên nhau."", ""B. Khi mô hình có cấu trúc phức tạp: nhiều đầu vào (Multiple Inputs), nhiều đầu ra (Multiple Outputs), hoặc có các kết nối tắt (như ResNet)."", ""C. Khi bạn mới bắt đầu học."", ""D. Khi dữ liệu là ảnh.""]",B,"[""10.4_Keras_Functional""]",medium
Chg10_Q9_MCQ,"Trong Keras, phương thức 'model.summary()' giúp bạn xem thông tin gì?","[""A. Xem code nguồn của Keras."", ""B. Xem cấu trúc mạng, hình dạng đầu ra của từng lớp (Output Shape) và số lượng tham số (Parameters) cần huấn luyện."", ""C. Xem biểu đồ Loss."", ""D. Xem dữ liệu huấn luyện.""]",B,"[""10.3_Keras_Sequential""]",easy
Chg10_Q10_MCQ,"Callback 'ModelCheckpoint' trong Keras có tác dụng gì?","[""A. Tự động dừng huấn luyện nếu không cải thiện."", ""B. Lưu lại mô hình (hoặc trọng số) định kỳ trong quá trình huấn luyện (ví dụ: chỉ lưu mô hình tốt nhất - save_best_only)."", ""C. Thay đổi tốc độ học."", ""D. Vẽ biểu đồ.""]",B,"[""10.3_Keras_Sequential""]",medium

Chg10_Pra_Q3,"Khi tải bộ dữ liệu Fashion MNIST bằng lệnh `keras.datasets.fashion_mnist.load_data()`, dữ liệu trả về có cấu trúc như thế nào?","[""A. Trả về một DataFrame lớn."", ""B. Trả về 2 tuple: `(X_train_full, y_train_full), (X_test, y_test)` chứa các mảng NumPy."", ""C. Trả về các file ảnh .jpg."", ""D. Trả về các tensor của TensorFlow.""]",B,"[""10.5_ThucHanh_Keras_Basic""]",easy
Chg10_Pra_Q4,"Trước khi đưa dữ liệu ảnh (pixel 0-255) vào mạng nơ-ron, bước tiền xử lý quan trọng nhất cần làm là gì để Gradient Descent hội tụ nhanh hơn?","[""A. Chuyển ảnh sang đen trắng."", ""B. Chia tất cả giá trị pixel cho 255.0 để đưa về khoảng [0, 1] (Scaling)."", ""C. Tăng độ tương phản."", ""D. Xoay ảnh ngẫu nhiên.""]",B,"[""10.5_ThucHanh_Keras_Basic""]",medium
Chg10_Pra_Q5,"Trong Keras Sequential API, lớp `Flatten(input_shape=[28, 28])` có tác dụng gì?","[""A. Làm mờ ảnh."", ""B. Biến đổi mảng ảnh 2D (28x28) thành mảng 1D (784) để có thể đưa vào lớp Dense tiếp theo."", ""C. Giảm kích thước ảnh xuống còn 14x14."", ""D. Tăng số lượng đặc trưng.""]",B,"[""10.5_ThucHanh_Keras_Basic""]",easy
Chg10_Pra_Q6,"Khi biên dịch mô hình (compile) cho bài toán phân loại 10 lớp (nhãn là số nguyên 0-9), nếu bạn chọn `loss='categorical_crossentropy'`, bạn sẽ gặp lỗi hoặc kết quả sai. Bạn phải chọn hàm loss nào mới đúng (nếu không one-hot encode nhãn)?","[""A. `binary_crossentropy`"", ""B. `sparse_categorical_crossentropy`"", ""C. `mean_squared_error`"", ""D. `hinge`""]",B,"[""10.5_ThucHanh_Keras_Basic""]",medium
Chg10_Pra_Q7,"Đối tượng `history` được trả về từ lệnh `history = model.fit(...)` chứa thông tin quan trọng gì trong thuộc tính `history.history`?","[""A. Trọng số của mô hình."", ""B. Một từ điển (dictionary) chứa các giá trị loss và metrics (như accuracy) trên tập huấn luyện và tập kiểm tra qua từng epoch, dùng để vẽ biểu đồ Learning Curve."", ""C. Dự đoán của mô hình."", ""D. Cấu trúc mô hình.""]",B,"[""10.5_ThucHanh_Keras_Basic""]",medium
Chg10_Pra_Q8,"Cú pháp nào sau đây là đúng để định nghĩa một lớp ẩn kết nối với lớp đầu vào `input_` bằng Functional API?","[""A. hidden1 = Dense(30, activation='relu')(input_)"", ""B. hidden1 = input_(Dense(30, activation='relu'))"", ""C. hidden1 = Dense(30, activation='relu', input=input_)"", ""D. model.add(Dense(30, activation='relu'))""]",A,"[""10.5_ThucHanh_Keras_Basic""]",hard
Chg10_Pra_Q9,"Khi sử dụng Callback `EarlyStopping`, tham số `restore_best_weights=True` có ý nghĩa gì?","[""A. Tự động lưu mô hình ra file."", ""B. Sau khi dừng huấn luyện, nó sẽ khôi phục lại trạng thái trọng số của mô hình tại epoch tốt nhất (thay vì giữ lại trọng số ở epoch cuối cùng - lúc mô hình có thể đã bị Overfit)."", ""C. Reset trọng số về 0."", ""D. Tăng tốc độ học.""]",B,"[""10.5_ThucHanh_Keras_Basic""]",medium
Chg10_Pra_Q10,"Lệnh `model.save('my_model.h5')` (hoặc `.keras`) lưu những gì?","[""A. Chỉ lưu trọng số (weights)."", ""B. Chỉ lưu kiến trúc mạng."", ""C. Lưu toàn bộ: Kiến trúc mạng, Trọng số, Cấu hình Compile (loss, optimizer) và Trạng thái của Optimizer (để có thể tiếp tục train đúng chỗ đã dừng)."", ""D. Chỉ lưu kết quả dự đoán.""]",C,"[""10.5_ThucHanh_Keras_Basic""]",hard

Chg11_Q1_MCQ,"Vấn đề 'Biến mất đạo hàm' (Vanishing Gradient) gây ra hậu quả gì khi huấn luyện mạng nơ-ron sâu?","[""A. Các lớp gần đầu ra (Output) học quá nhanh."", ""B. Các lớp gần đầu vào (Input) nhận được tín hiệu sai số quá nhỏ, khiến trọng số của chúng hầu như không được cập nhật, làm mạng không thể học được các đặc trưng cấp thấp."", ""C. Mạng bị bùng nổ bộ nhớ."", ""D. Đạo hàm tăng lên vô hạn.""]",B,"[""11.1_Vanishing_Exploding""]",medium
Chg11_Q2_MCQ,"Để giải quyết vấn đề Vanishing Gradient với hàm kích hoạt ReLU, chiến lược khởi tạo trọng số (Initialization) nào được khuyên dùng?","[""A. Khởi tạo tất cả bằng 0."", ""B. Glorot (Xavier) Initialization."", ""C. He Initialization (He Normal/Uniform)."", ""D. Khởi tạo ngẫu nhiên cực lớn.""]",C,"[""11.2_KhoiTao_HeSo""]",medium
Chg11_Q3_MCQ,"Hiện tượng 'Dying ReLU' (Neuron chết) là gì?","[""A. Neuron luôn cho đầu ra là 0 vì trọng số cập nhật sao cho tổng đầu vào luôn âm. Khi đó, đạo hàm của nó cũng bằng 0 và nó ngừng học vĩnh viễn."", ""B. Neuron bị xóa khỏi mạng."", ""C. Neuron có giá trị quá lớn."", ""D. Neuron hoạt động tốt hơn.""]",A,"[""11.3_Activation_Functions""]",hard
Chg11_Q4_MCQ,"Hàm kích hoạt nào sau đây có khả năng 'Tự chuẩn hóa' (Self-Normalizing) giúp mạng nơ-ron sâu hội tụ ổn định mà không cần Batch Norm?","[""A. Sigmoid."", ""B. ReLU."", ""C. SELU (Scaled Exponential Linear Unit)."", ""D. Tanh.""]",C,"[""11.3_Activation_Functions""]",hard
Chg11_Q5_MCQ,"Kỹ thuật 'Batch Normalization' (Chuẩn hóa theo lô) hoạt động như thế nào?","[""A. Nó chuẩn hóa dữ liệu đầu vào của toàn bộ tập dữ liệu một lần duy nhất."", ""B. Tại mỗi lớp, nó chuẩn hóa các đầu vào (inputs) của lớp đó theo từng mini-batch (trừ trung bình, chia phương sai), sau đó học thêm tham số tỷ lệ và dịch chuyển."", ""C. Nó loại bỏ các neuron yếu."", ""D. Nó chuẩn hóa trọng số (Weights) thay vì đầu ra.""]",B,"[""11.4_Batch_Normalization""]",medium
Chg11_Q6_MCQ,"Lợi ích lớn nhất của Batch Normalization là gì?","[""A. Giảm số lượng tham số."", ""B. Giải quyết triệt để vấn đề Vanishing Gradient, cho phép sử dụng tốc độ học (Learning Rate) lớn hơn nhiều và giúp mạng ít nhạy cảm với việc khởi tạo trọng số."", ""C. Làm cho mạng nhỏ lại."", ""D. Thay thế hoàn toàn Dropout.""]",B,"[""11.4_Batch_Normalization""]",medium
Chg11_Q7_MCQ,"Kỹ thuật 'Gradient Clipping' thường được dùng để xử lý vấn đề gì?","[""A. Vanishing Gradient (Biến mất đạo hàm)."", ""B. Exploding Gradient (Bùng nổ đạo hàm) - thường gặp trong mạng RNN."", ""C. Overfitting."", ""D. Underfitting.""]",B,"[""11.1_Vanishing_Exploding""]",medium
Chg11_Q8_MCQ,"Thuật toán tối ưu 'Momentum' cải thiện Gradient Descent thường bằng cách nào?","[""A. Nó chỉ quan tâm đến độ dốc hiện tại."", ""B. Nó mô phỏng một quả bóng lăn xuống đồi: cộng dồn các vector gradient trước đó (quán tính) để tăng tốc độ khi xuống dốc và vượt qua các điểm cực tiểu địa phương (local minima)."", ""C. Nó giảm tốc độ học theo thời gian."", ""D. Nó reset trọng số ngẫu nhiên.""]",B,"[""11.5_Optimizers_Adam""]",easy
Chg11_Q9_MCQ,"Adam Optimizer là sự kết hợp của hai ý tưởng nào?","[""A. Momentum và RMSProp."", ""B. Batch GD và Stochastic GD."", ""C. L1 và L2 Regularization."", ""D. ReLU và Sigmoid.""]",A,"[""11.5_Optimizers_Adam""]",medium
Chg11_Q10_MCQ,"Kỹ thuật 'Dropout' chống Overfitting như thế nào trong quá trình huấn luyện?","[""A. Tại mỗi bước huấn luyện, nó ngẫu nhiên tắt (bỏ qua) một tỷ lệ phần trăm các neuron (ví dụ 20%), khiến mạng không thể phụ thuộc vào bất kỳ neuron đơn lẻ nào."", ""B. Nó xóa vĩnh viễn các neuron."", ""C. Nó giảm giá trị trọng số về 0."", ""D. Nó thêm dữ liệu nhiễu vào đầu vào.""]",A,"[""11.6_Regularization_Dropout""]",easy
Chg11_Q11_MCQ,"Điều gì quan trọng cần nhớ về Dropout khi chuyển sang giai đoạn Kiểm tra/Dự đoán (Testing/Inference)?","[""A. Dropout phải được bật với tỷ lệ cao hơn."", ""B. Dropout phải được TẮT hoàn toàn (hoặc nhân đầu ra với tỷ lệ giữ lại) để sử dụng toàn bộ năng lực của mạng."", ""C. Dropout không ảnh hưởng gì."", ""D. Phải huấn luyện lại mô hình.""]",B,"[""11.6_Regularization_Dropout""]",hard
Chg11_Q12_MCQ,"'Learning Rate Scheduling' (Lịch trình tốc độ học) như 'Performance Scheduling' có tác dụng gì?","[""A. Tăng tốc độ học lên vô hạn."", ""B. Giảm dần tốc độ học khi sai số (validation error) ngừng cải thiện, giúp mô hình hội tụ sâu hơn vào điểm cực tiểu."", ""C. Giữ tốc độ học không đổi."", ""D. Thay đổi thuật toán tối ưu.""]",B,"[""11.5_Optimizers_Adam""]",medium

Chg11_Pra_Q3,"Để sử dụng chiến lược khởi tạo 'He Normal' (được khuyên dùng cho hàm kích hoạt ReLU để tránh Vanishing Gradient), bạn cấu hình lớp Dense như thế nào?","[""A. Dense(100, activation='relu', kernel_initializer='he_normal')"", ""B. Dense(100, activation='relu', kernel_initializer='zeros')"", ""C. Dense(100, activation='relu', kernel_initializer='glorot_uniform')"", ""D. Dense(100, activation='relu', kernel_initializer='random_normal') ""]",A,"[""11.7_ThucHanh_DeepNet""]",medium
Chg11_Pra_Q4,"Khi thêm lớp `BatchNormalization()` vào TRƯỚC hàm kích hoạt của lớp Dense (ví dụ: Dense -> BatchNom -> Activation), bạn nên thiết lập tham số nào cho lớp Dense để tránh lãng phí tham số (vì Batch Norm đã có tham số dịch chuyển beta)?","[""A. use_bias=False"", ""B. use_bias=True"", ""C. activation=None"", ""D. kernel_regularizer=None""]",A,"[""11.7_ThucHanh_DeepNet""]",hard
Chg11_Pra_Q5,"Để sử dụng biến thể hàm kích hoạt `LeakyReLU` trong Keras Sequential API, cách viết nào là chính xác (theo các phiên bản Keras cổ điển được dạy trong sách)?","[""A. model.add(Dense(10, activation='leaky_relu'))"", ""B. model.add(Dense(10)); model.add(LeakyReLU(alpha=0.2))"", ""C. model.add(Dense(10, activation='relu', alpha=0.2))"", ""D. model.add(LeakyReLU(10))""]",B,"[""11.7_ThucHanh_DeepNet""]",medium
Chg11_Pra_Q6,"Khi khởi tạo một Optimizer (ví dụ SGD), tham số nào giúp ngăn chặn vấn đề 'Bùng nổ đạo hàm' (Exploding Gradients), thường gặp trong mạng RNN?","[""A. momentum=0.9"", ""B. nesterov=True"", ""C. clipvalue=1.0 (hoặc clipnorm=1.0)"", ""D. decay=1e-4""]",C,"[""11.7_ThucHanh_DeepNet""]",medium
Chg11_Pra_Q7,"Callback `ReduceLROnPlateau` hoạt động như thế nào?","[""A. Tăng tốc độ học lên gấp đôi sau mỗi epoch."", ""B. Giảm tốc độ học (nhân với một hệ số factor, ví dụ 0.5) nếu loss trên tập validation không cải thiện sau một số epoch nhất định (patience)."", ""C. Giảm tốc độ học theo hàm mũ."", ""D. Dừng huấn luyện ngay lập tức.""]",B,"[""11.7_ThucHanh_DeepNet""]",medium
Chg11_Pra_Q8,"Để áp dụng `L2 Regularization` với hệ số 0.01 cho trọng số (weights) của lớp Dense, bạn dùng cú pháp nào?","[""A. Dense(100, kernel_regularizer=keras.regularizers.l2(0.01))"", ""B. Dense(100, activity_regularizer=keras.regularizers.l2(0.01))"", ""C. Dense(100, bias_regularizer=keras.regularizers.l2(0.01))"", ""D. Dense(100, dropout=0.01)""]",A,"[""11.7_ThucHanh_DeepNet""]",medium
Chg11_Pra_Q9,"Khi thực hiện Transfer Learning, lệnh `layer.trainable = False` có tác dụng gì?","[""A. Xóa lớp đó khỏi mô hình."", ""B. Đóng băng trọng số của lớp đó, ngăn không cho Gradient Descent cập nhật chúng trong quá trình huấn luyện (thường dùng cho các lớp dưới cùng của mô hình pre-trained)."", ""C. Làm cho lớp đó học nhanh hơn."", ""D. Reset trọng số của lớp đó.""]",B,"[""11.7_ThucHanh_DeepNet""]",easy
Chg11_Pra_Q10,"Lớp `Dropout(rate=0.2)` hoạt động như thế nào trong giai đoạn suy luận (Inference/Testing)?","[""A. Nó tiếp tục tắt 20% neuron ngẫu nhiên."", ""B. Nó không làm gì cả (hoạt động như một lớp identity), nhưng các đầu vào có thể được nhân tỉ lệ để bù đắp cho việc thiếu hụt trong lúc train."", ""C. Nó tắt toàn bộ neuron."", ""D. Nó báo lỗi.""]",B,"[""11.7_ThucHanh_DeepNet""]",medium

Chg12_Q1_MCQ,"Sự khác biệt chính giữa TensorFlow 'Tensors' và NumPy 'Arrays' là gì?","[""A. Không có sự khác biệt nào."", ""B. Tensors là bất biến (immutable) và có thể chạy trên GPU/TPU để tăng tốc tính toán, trong khi NumPy Arrays thì không."", ""C. Tensors chỉ chứa số nguyên."", ""D. NumPy Arrays nhanh hơn Tensors.""]",B,"[""12.1_Tensor_Operations""]",easy
Chg12_Q2_MCQ,"Để tính đạo hàm tự động (Automatic Differentiation) trong các vòng lặp huấn luyện tùy chỉnh, TensorFlow sử dụng công cụ nào?","[""A. tf.gradient()."", ""B. tf.GradientTape context manager: Nó ghi lại mọi phép toán xảy ra bên trong khối 'with' để sau đó có thể truy ngược và tính đạo hàm."", ""C. Backpropagation thủ công."", ""D. tf.keras.optimizers.""]",B,"[""12.3_Custom_Training_Loop""]",medium
Chg12_Q3_MCQ,"Khi tự viết một lớp tùy chỉnh (Custom Layer) kế thừa từ `keras.layers.Layer`, phương thức `build(input_shape)` thường được dùng để làm gì?","[""A. Thực hiện tính toán chính (Forward pass)."", ""B. Khởi tạo các trọng số (Weights) của lớp, vì lúc này ta mới biết chính xác kích thước đầu vào (input_shape)."", ""C. Lưu mô hình."", ""D. Tính toán Loss.""]",B,"[""12.2_Custom_Layers_Models""]",hard
Chg12_Q4_MCQ,"Phương thức `call(inputs)` trong Custom Layer/Model đảm nhận nhiệm vụ gì?","[""A. Định nghĩa kiến trúc mạng."", ""B. Thực hiện các phép tính toán logic của lớp (Forward pass) trên dữ liệu đầu vào."", ""C. Tải dữ liệu."", ""D. Vẽ đồ thị.""]",B,"[""12.2_Custom_Layers_Models""]",medium
Chg12_Q5_MCQ,"Decorator `@tf.function` có tác dụng gì khi đặt trước một hàm Python?","[""A. Biến hàm đó thành một lớp."", ""B. Chuyển đổi hàm Python đó thành TensorFlow Graph (Đồ thị tính toán) để tối ưu hóa hiệu năng và cho phép chạy trên thiết bị không có Python (như di động)."", ""C. Làm chậm hàm đó lại để debug."", ""D. Tự động thêm chú thích.""]",B,"[""12.1_Tensor_Operations""]",hard

Chg12_Pra_Q3,"Khi làm việc với Tensor, câu lệnh `tf.constant([[1., 2., 3.]])` tạo ra một đối tượng có tính chất gì khác biệt so với `tf.Variable`?","[""A. Nó có thể thay đổi giá trị (mutable)."", ""B. Nó là bất biến (immutable) - không thể thay đổi giá trị sau khi khởi tạo."", ""C. Nó không thể chạy trên GPU."", ""D. Nó chỉ chứa số nguyên.""]",B,"[""12.4_ThucHanh_CustomTF""]",easy
Chg12_Pra_Q4,"Để chuyển đổi một mảng NumPy sang Tensor, ta dùng lệnh nào?","[""A. tf.to_tensor(array)"", ""B. tf.convert_to_tensor(array)"", ""C. tf.from_numpy(array)"", ""D. array.to_tf()""]",B,"[""12.4_ThucHanh_CustomTF""]",easy
Chg12_Pra_Q5,"Trong phương thức `build(self, input_shape)` của một Custom Layer, lệnh `self.add_weight(...)` dùng để làm gì?","[""A. Thêm một lớp con."", ""B. Tạo và khởi tạo các biến trọng số (trainable weights) cho lớp, vì lúc này ta mới biết kích thước đầu vào."", ""C. Tính toán đầu ra."", ""D. Thêm bias.""]",B,"[""12.4_ThucHanh_CustomTF""]",medium
Chg12_Pra_Q6,"Khi viết Custom Layer có sử dụng `Dropout` hoặc `BatchNormalization`, tại sao phương thức `call(self, inputs, training=None)` cần tham số `training`?","[""A. Để báo cho lớp biết nó đang chạy trong giai đoạn Huấn luyện (Training) hay Suy luận (Inference), vì hành vi của Dropout/Batch Norm khác nhau ở 2 giai đoạn này."", ""B. Để bật tắt GPU."", ""C. Để tính đạo hàm."", ""D. Để lưu mô hình.""]",A,"[""12.4_ThucHanh_CustomTF""]",hard
Chg12_Pra_Q7,"Để tính đạo hàm bậc 2 (second-order derivative), bạn cần làm gì với `tf.GradientTape`?","[""A. Không thể tính được."", ""B. Lồng hai khối `with tf.GradientTape() as tape:` vào nhau (một tape bên ngoài ghi lại quá trình tính đạo hàm của tape bên trong)."", ""C. Gọi hàm `tape.gradient()` hai lần liên tiếp."", ""D. Dùng `tf.hessian()`.""]",B,"[""12.4_ThucHanh_CustomTF""]",hard
Chg12_Pra_Q8,"Mặc định, `tf.GradientTape` chỉ giữ lại các phép toán cho một lần gọi `.gradient()`. Để tính đạo hàm nhiều lần (ví dụ: tính đạo hàm cho 2 hàm mất mát khác nhau trên cùng một biến), bạn cần khởi tạo tape như thế nào?","[""A. tf.GradientTape(persistent=True)"", ""B. tf.GradientTape(retain_graph=True)"", ""C. tf.GradientTape(multi_call=True)"", ""D. tf.GradientTape(watch_accessed_variables=False)""]",A,"[""12.4_ThucHanh_CustomTF""]",medium
Chg12_Pra_Q9,"Decorator `@tf.function` giúp tăng tốc độ thực thi hàm Python bằng cách nào?","[""A. Chạy đa luồng."", ""B. Chuyển đổi (Trace) hàm Python thành Đồ thị tính toán TensorFlow (Static Graph) để tối ưu hóa các phép toán (như gộp node, bỏ qua tính toán thừa)."", ""C. Chuyển code sang C++."", ""D. Tự động phân tán sang nhiều máy chủ.""]",B,"[""12.4_ThucHanh_CustomTF""]",medium
Chg12_Pra_Q10,"Để lưu và tải lại một Custom Model (có chứa Custom Layer) bằng phương thức `.save()`, bạn cần cài đặt phương thức nào trong lớp của mình để Keras biết cách lưu các tham số cấu hình (hyperparameters)?","[""A. `get_config(self)`"", ""B. `save_config(self)`"", ""C. `__init__(self)`"", ""D. `serialize(self)`""]",A,"[""12.4_ThucHanh_CustomTF""]",hard

Chg13_Q1_MCQ,"Mục tiêu chính của `tf.data` API trong TensorFlow là gì?","[""A. Để vẽ biểu đồ dữ liệu."", ""B. Để xây dựng các đường ống dữ liệu (data pipelines) hiệu năng cao, linh hoạt, có khả năng xử lý dữ liệu lớn hơn bộ nhớ RAM và hỗ trợ đọc dữ liệu song song."", ""C. Để lưu trữ dữ liệu dưới dạng văn bản."", ""D. Để thay thế Pandas.""]",B,"[""13.1_TF_Data_API""]",easy
Chg13_Q2_MCQ,"Trong kỹ thuật 'ETL' (Extract-Transform-Load) của tf.data, bước 'Prefetching' (Tải trước) giúp tăng tốc độ huấn luyện như thế nào?","[""A. Nó giảm kích thước của mô hình."", ""B. Nó cho phép CPU chuẩn bị lô dữ liệu tiếp theo (batch n+1) trong khi GPU đang bận huấn luyện lô hiện tại (batch n), giúp loại bỏ thời gian chờ đợi của GPU."", ""C. Nó nén dữ liệu lại."", ""D. Nó bỏ qua các dữ liệu bị lỗi.""]",B,"[""13.1_TF_Data_API""]",medium
Chg13_Q3_MCQ,"Khi sử dụng `dataset.shuffle(buffer_size)`, nếu `buffer_size` nhỏ hơn nhiều so với tổng số lượng mẫu dữ liệu, điều gì sẽ xảy ra?","[""A. Dữ liệu sẽ không được xáo trộn chút nào."", ""B. Dữ liệu chỉ được xáo trộn cục bộ (trong phạm vi bộ đệm), không phải xáo trộn hoàn toàn ngẫu nhiên trên toàn bộ tập dữ liệu, có thể dẫn đến việc mô hình học theo thứ tự sai lệch."", ""C. Chương trình sẽ báo lỗi."", ""D. Tốc độ đọc sẽ nhanh hơn.""]",B,"[""13.1_TF_Data_API""]",medium
Chg13_Q4_MCQ,"Định dạng TFRecord là gì?","[""A. Một định dạng XML phức tạp."", ""B. Một định dạng nhị phân (binary) đơn giản chứa một chuỗi các bản ghi (records), thường sử dụng Protocol Buffers, được tối ưu hóa để TensorFlow đọc tuần tự cực nhanh."", ""C. Một cơ sở dữ liệu SQL."", ""D. Một file bảng tính Excel.""]",B,"[""13.2_TFRecord_Format""]",medium
Chg13_Q5_MCQ,"Tại sao ta nên sử dụng TFRecord cho các bộ dữ liệu lớn (Big Data)?","[""A. Vì nó dễ đọc bằng mắt thường."", ""B. Vì nó giúp việc đọc dữ liệu hiệu quả hơn (tránh overhead khi mở hàng nghìn file nhỏ), hỗ trợ nén (GZIP), và tích hợp hoàn hảo với `tf.data` để phân tán dữ liệu."", ""C. Vì nó tự động làm sạch dữ liệu."", ""D. Vì nó hỗ trợ chỉnh sửa dữ liệu trực tiếp.""]",B,"[""13.2_TFRecord_Format""]",medium
Chg13_Q6_MCQ,"Protocol Buffers (protobuf) được sử dụng trong TFRecord đóng vai trò gì?","[""A. Mã hóa video."", ""B. Là cơ chế serilization (tuần tự hóa) dữ liệu cấu trúc thành chuỗi byte nhỏ gọn, đa nền tảng, giúp định nghĩa cấu trúc dữ liệu (`Example`, `Feature`) một cách chặt chẽ."", ""C. Nén ảnh JPEG."", ""D. Giao thức mạng.""]",B,"[""13.2_TFRecord_Format""]",hard
Chg13_Q7_MCQ,"Lợi ích lớn nhất của việc đưa các lớp Keras Preprocessing (như `Normalization`, `TextVectorization`) vào TRONG mô hình (end-to-end model) là gì?","[""A. Tăng tốc độ GPU."", ""B. Đảm bảo tính nhất quán giữa lúc huấn luyện (training) và lúc dự đoán (inference/serving), tránh lỗi 'Training-Serving Skew' do code tiền xử lý không đồng bộ."", ""C. Giảm dung lượng file mô hình."", ""D. Giúp mô hình dễ debug hơn.""]",B,"[""13.3_Keras_Preprocessing_Layers""]",hard
Chg13_Q8_MCQ,"Lớp `TextVectorization` thực hiện những công việc gì?","[""A. Chỉ tách từ."", ""B. Chuẩn hóa văn bản (lowercase, xóa dấu câu), tách từ (tokenization), và lập chỉ mục từ vựng (indexing) để biến câu thành chuỗi số nguyên."", ""C. Dịch văn bản."", ""D. Kiểm tra chính tả.""]",B,"[""13.3_Keras_Preprocessing_Layers""]",medium
Chg13_Q9_MCQ,"Phương thức `.adapt()` trong các lớp Preprocessing dùng để làm gì?","[""A. Để huấn luyện mô hình."", ""B. Để tính toán các thống kê cần thiết từ dữ liệu huấn luyện (ví dụ: tính trung bình/phương sai cho `Normalization`, hoặc xây dựng từ điển cho `TextVectorization`) trước khi bắt đầu train mô hình."", ""C. Để dự đoán kết quả."", ""D. Để tải dữ liệu.""]",B,"[""13.3_Keras_Preprocessing_Layers""]",easy
Chg13_Q10_MCQ,"Khi nào bạn nên sử dụng `One-Hot Encoding` (via `CategoryEncoding` hoặc `StringLookup`) so với `Embedding`?","[""A. Khi số lượng hạng mục (categories) rất lớn (ví dụ: > 10,000)."", ""B. Khi số lượng hạng mục nhỏ (ví dụ: < 10) và các hạng mục không có quan hệ ngữ nghĩa phức tạp."", ""C. Luôn luôn dùng One-Hot Encoding."", ""D. Khi dữ liệu là văn bản dài.""]",B,"[""13.3_Keras_Preprocessing_Layers""]",medium

Chg13_Pra_Q3,"Khi đọc dữ liệu từ nhiều file CSV lớn, phương thức `dataset.interleave()` giúp tăng tốc độ như thế nào so với `flat_map`?","[""A. Nó đọc tuần tự từng file một."", ""B. Nó đọc đan xen (interleaves) các mẫu dữ liệu từ nhiều file cùng lúc (song song), đặc biệt hiệu quả khi kết hợp với `num_parallel_calls=tf.data.AUTOTUNE`."", ""C. Nó gộp tất cả file thành một file lớn."", ""D. Nó xóa các dòng trùng lặp.""]",B,"[""13.4_ThucHanh_DataAPI""]",hard
Chg13_Pra_Q4,"Lớp `TextVectorization` có tham số `standardize`. Mặc định (khi không chỉnh sửa), tham số này thực hiện công việc gì?","[""A. Không làm gì cả."", ""B. Chuyển văn bản thành chữ thường (lowercase) và loại bỏ dấu câu (punctuation)."", ""C. Dịch văn bản sang tiếng Anh."", ""D. Sửa lỗi chính tả.""]",B,"[""13.4_ThucHanh_DataAPI""]",medium
Chg13_Pra_Q5,"Trước khi sử dụng lớp `Normalization()` hoặc `TextVectorization()` trong mô hình, bạn BẮT BUỘC phải gọi phương thức nào để lớp này học được thống kê dữ liệu (trung bình/phương sai hoặc từ vựng)?","[""A. layer.fit(data)"", ""B. layer.adapt(data)"", ""C. layer.compile(data)"", ""D. layer.build(data)""]",B,"[""13.4_ThucHanh_DataAPI""]",medium
Chg13_Pra_Q6,"Để chia một `tf.data.Dataset` thành tập Train (80%) và Valid (20%) mà không cần tải hết vào RAM, bạn dùng cặp lệnh nào?","[""A. `train = dataset[:800]`, `valid = dataset[800:]`"", ""B. `train = dataset.take(train_size)`, `valid = dataset.skip(train_size)`"", ""C. `train_test_split(dataset)`"", ""D. `dataset.split(0.8)`""]",B,"[""13.4_ThucHanh_DataAPI""]",medium
Chg13_Pra_Q7,"Lớp `StringLookup` xử lý các giá trị hạng mục (categorical) chưa từng xuất hiện trong tập huấn luyện (Out-of-Vocabulary / OOV) như thế nào?","[""A. Báo lỗi chương trình."", ""B. Gán chúng vào một 'thùng' (bucket) đặc biệt dành cho OOV (thường có chỉ số là 0 hoặc 1)."", ""C. Tự động bỏ qua các mẫu đó."", ""D. Thay thế bằng từ gần nhất.""]",B,"[""13.4_ThucHanh_DataAPI""]",medium
Chg13_Pra_Q8,"Khi phân tích (parse) một mẫu dữ liệu TFRecord bằng hàm `tf.io.parse_single_example`, bạn cần cung cấp tham số `features` dưới dạng nào?","[""A. Một danh sách các tên cột."", ""B. Một từ điển (dictionary) ánh xạ tên đặc trưng sang `tf.io.FixedLenFeature` hoặc `tf.io.VarLenFeature` để định nghĩa kiểu dữ liệu và hình dạng."", ""C. Một DataFrame."", ""D. Một mảng NumPy.""]",B,"[""13.4_ThucHanh_DataAPI""]",hard
Chg13_Pra_Q9,"Để nhúng các vector từ (word embeddings) vào mô hình Keras, đầu ra của lớp `TextVectorization` (với output_mode='int') nên được nối trực tiếp vào lớp nào?","[""A. Dense()"", ""B. Embedding()"", ""C. LSTM()"", ""D. Conv1D()""]",B,"[""13.4_ThucHanh_DataAPI""]",easy
Chg13_Pra_Q10,"Lớp `CategoryEncoding` với `output_mode='one_hot'` khác gì so với `CategoryEncoding` với `output_mode='multi_hot'`?","[""A. Không khác gì."", ""B. 'one_hot' dùng khi mỗi mẫu chỉ thuộc 1 lớp duy nhất. 'multi_hot' dùng khi mỗi mẫu có thể thuộc nhiều lớp cùng lúc (ví dụ: một câu chứa nhiều từ khóa)."", ""C. 'one_hot' dùng cho ảnh, 'multi_hot' dùng cho văn bản."", ""D. 'one_hot' chạy chậm hơn.""]",B,"[""13.4_ThucHanh_DataAPI""]",medium

Chg14_Q1_MCQ,"Trong lớp Tích chập (Convolutional Layer), các 'Bộ lọc' (Filters/Kernels) đóng vai trò gì?","[""A. Giảm kích thước ảnh."", ""B. Trích xuất các đặc trưng cục bộ (Local Features) như cạnh, góc, đường nét từ ảnh đầu vào."", ""C. Xoay ảnh."", ""D. Chuyển ảnh thành đen trắng.""]",B,"[""14.2_Lop_TichChap_Pooling""]",medium
Chg14_Q2_MCQ,"Mục đích chính của lớp Pooling (ví dụ: Max Pooling) là gì?","[""A. Tăng độ phân giải của ảnh."", ""B. Giảm kích thước không gian của ảnh (Downsampling) để giảm khối lượng tính toán và giúp mô hình bất biến với các dịch chuyển nhỏ."", ""C. Tô màu cho ảnh."", ""D. Làm mịn ảnh.""]",B,"[""14.2_Lop_TichChap_Pooling""]",easy
Chg14_Q3_MCQ,"Kiến trúc ResNet (Residual Network) giải quyết vấn đề 'Biến mất đạo hàm' trong các mạng cực sâu bằng cách nào?","[""A. Sử dụng kết nối tắt (Skip/Residual Connections): Cộng trực tiếp đầu vào của lớp vào đầu ra của nó, giúp tín hiệu truyền xuyên suốt mạng dễ dàng hơn."", ""B. Sử dụng hàm kích hoạt Sigmoid."", ""C. Giảm số lượng lớp."", ""D. Tăng kích thước bộ lọc.""]",A,"[""14.3_KienTruc_CNN_KinhDien""]",hard
Chg14_Q4_MCQ,"Thuật toán YOLO (You Only Look Once) nổi tiếng trong bài toán nào?","[""A. Phân loại ảnh tĩnh."", ""B. Phát hiện vật thể (Object Detection) theo thời gian thực (Real-time), bằng cách coi việc phát hiện như một bài toán hồi quy duy nhất."", ""C. Tạo ảnh nghệ thuật."", ""D. Khôi phục ảnh cũ.""]",B,"[""14.5_PhatHien_VatThe_YOLO""]",medium
Chg14_Q5_MCQ,"Sự khác biệt giữa 'Phân loại ảnh' (Classification) và 'Phân đoạn ngữ nghĩa' (Semantic Segmentation) là gì?","[""A. Không có sự khác biệt."", ""B. Phân loại gán nhãn cho toàn bộ bức ảnh (ví dụ: 'chó'). Phân đoạn gán nhãn cho TỪNG PIXEL trong ảnh (ví dụ: các pixel này thuộc về con chó, các pixel kia thuộc về nền cỏ)."", ""C. Phân đoạn chỉ dùng cho video."", ""D. Phân loại khó hơn phân đoạn.""]",B,"[""14.6_PhanDoan_Anh""]",hard

Chg14_Pra_Q3,"Trong Keras, khi khai báo lớp `Conv2D`, tham số `padding='same'` có ý nghĩa gì?","[""A. Không thêm padding (giữ nguyên ảnh gốc)."", ""B. Tự động thêm lề (padding) bằng các số 0 xung quanh ảnh sao cho kích thước đầu ra (output size) BẰNG kích thước đầu vào (input size) (nếu strides=1)."", ""C. Chỉ thêm padding bên phải."", ""D. Cắt bớt ảnh.""]",B,"[""14.7_ThucHanh_CNN""]",easy
Chg14_Pra_Q4,"Để giảm kích thước không gian của ảnh đi một nửa (ví dụ từ 28x28 xuống 14x14) trong mạng CNN, ta thường dùng lớp nào?","[""A. Conv2D(filters=32, kernel_size=3)"", ""B. MaxPooling2D(pool_size=2)"", ""C. Flatten()"", ""D. Dense(128)""]",B,"[""14.7_ThucHanh_CNN""]",easy
Chg14_Pra_Q5,"Khi thực hiện Transfer Learning với mô hình Xception đã huấn luyện trên ImageNet, lệnh `base_model.trainable = False` dùng để làm gì?","[""A. Tắt GPU."", ""B. Đóng băng toàn bộ trọng số của mô hình gốc để chúng không bị phá vỡ (không bị cập nhật) trong những epoch đầu tiên khi ta huấn luyện các lớp Dense mới thêm vào."", ""C. Xóa các lớp của mô hình."", ""D. Tăng tốc độ dự báo.""]",B,"[""14.7_ThucHanh_CNN""]",medium
Chg14_Pra_Q6,"Hàm `preprocess_input` đi kèm với các mô hình trong `keras.applications` (ví dụ `keras.applications.resnet50.preprocess_input`) có tác dụng gì quan trọng?","[""A. Nó thay đổi kích thước ảnh."", ""B. Nó thực hiện các bước tiền xử lý đặc thù mà mô hình gốc yêu cầu (ví dụ: chuẩn hóa pixel về khoảng [-1, 1] hoặc trừ đi trung bình màu của ImageNet). Nếu không dùng, mô hình sẽ dự đoán sai hoàn toàn."", ""C. Nó xoay ảnh."", ""D. Nó chuyển ảnh sang đen trắng.""]",B,"[""14.7_ThucHanh_CNN""]",hard
Chg14_Pra_Q7,"Lớp `GlobalAveragePooling2D()` thường được dùng thay thế cho lớp `Flatten()` ở cuối phần Feature Extractor của CNN hiện đại. Nó hoạt động như thế nào?","[""A. Nó lấy giá trị lớn nhất của mỗi feature map."", ""B. Nó tính trung bình cộng của toàn bộ các giá trị trong MỖI feature map (kết quả là một vector có số chiều bằng số channels), giúp giảm mạnh số lượng tham số so với Flatten."", ""C. Nó nối tất cả pixel lại thành 1 hàng."", ""D. Nó xóa bỏ các feature map yếu.""]",B,"[""14.7_ThucHanh_CNN""]",medium
Chg14_Pra_Q8,"Để tăng cường dữ liệu ảnh (Data Augmentation) trực tiếp trong mô hình Keras (để chạy được trên GPU), ta nên dùng các lớp nào?","[""A. `ImageDataGenerator` (cách cũ)."", ""B. Các lớp trong `keras.layers` như `RandomFlip`, `RandomRotation`, `RandomZoom`."", ""C. Dùng thư viện OpenCV bên ngoài."", ""D. Dùng Photoshop.""]",B,"[""14.7_ThucHanh_CNN""]",medium
Chg14_Pra_Q9,"Khi xây dựng một mạng CNN từ đầu cho bài toán phân loại ảnh màu 32x32, `input_shape` phải là bao nhiêu?","[""A. [32, 32]"", ""B. [32, 32, 1]"", ""C. [32, 32, 3]"", ""D. [3, 32, 32]""]",C,"[""14.7_ThucHanh_CNN""]",easy
Chg14_Pra_Q10,"Trong bài toán Phát hiện vật thể (Object Detection), đầu ra của mô hình thường bao gồm những gì?","[""A. Chỉ là nhãn lớp (Class label)."", ""B. Nhãn lớp (Class probabilities) VÀ tọa độ khung bao (Bounding box coordinates: x, y, w, h)."", ""C. Một bức ảnh mới."", ""D. Segmentation mask.""]",B,"[""14.7_ThucHanh_CNN""]",medium

Chg15_Q1_MCQ,"Tại sao mạng RNN truyền thống (Simple RNN) gặp khó khăn với các chuỗi dữ liệu dài (ví dụ: đoạn văn 100 từ)?","[""A. Vì nó tốn quá nhiều RAM."", ""B. Vì vấn đề 'Trí nhớ ngắn hạn' (Short-term memory): Gradient bị biến mất qua thời gian (Vanishing Gradient through time), khiến mạng quên mất thông tin ở đầu chuỗi."", ""C. Vì nó chỉ xử lý được số."", ""D. Vì nó chạy quá nhanh.""]",B,"[""15.1_Mang_RNN_CoBan""]",medium
Chg15_Q2_MCQ,"LSTM (Long Short-Term Memory) giải quyết vấn đề của RNN bằng cơ chế nào?","[""A. Sử dụng các 'Cổng' (Gates) như Cổng quên, Cổng nhập, Cổng xuất để chủ động kiểm soát luồng thông tin (cái nào cần nhớ, cái nào cần quên) qua thời gian dài."", ""B. Sử dụng nhiều lớp Convolution."", ""C. Loại bỏ các vòng lặp hồi quy."", ""D. Chỉ dùng hàm ReLU.""]",A,"[""15.4_LSTM_GRU""]",medium
Chg15_Q3_MCQ,"Trong dự báo chuỗi thời gian (Time Series), kỹ thuật 'Differencing' (Lấy sai phân) thường được dùng để làm gì?","[""A. Để tăng dữ liệu."", ""B. Để loại bỏ xu hướng (Trend) và tính mùa vụ (Seasonality), làm cho chuỗi dữ liệu trở nên dừng (stationary) giúp mô hình dễ học hơn."", ""C. Để làm mượt biểu đồ."", ""D. Để chuyển đổi sang tần miền số.""]",B,"[""15.3_DuBao_ThoiGian""]",hard

Chg15_Pra_Q3,"Khi sử dụng `dataset.window(window_length, shift=1)` của `tf.data` để tạo dữ liệu chuỗi thời gian, kết quả trả về là một 'Dataset of Datasets'. Để biến nó thành một dataset phẳng chứa các Tensor (batch) có thể huấn luyện được, bạn cần gọi phương thức nào tiếp theo?","[""A. .flat_map(lambda window: window.batch(window_length))"", ""B. .map()"", ""C. .flatten()"", ""D. .zip() ""]",A,"[""15.5_ThucHanh_RNN""]",hard
Chg15_Pra_Q4,"Lớp `TimeDistributed(Dense(10))` hoạt động như thế nào khi áp dụng lên đầu ra của một lớp RNN có `return_sequences=True`?","[""A. Nó chỉ áp dụng lớp Dense cho bước thời gian cuối cùng."", ""B. Nó áp dụng cùng một lớp Dense (chia sẻ trọng số) cho TỪNG bước thời gian (time step) của chuỗi đầu vào một cách độc lập."", ""C. Nó tính trung bình cộng của tất cả các bước."", ""D. Nó làm chậm quá trình huấn luyện.""]",B,"[""15.5_ThucHanh_RNN""]",medium
Chg15_Pra_Q5,"Khi huấn luyện RNN với chế độ `stateful=True` (để mô hình ghi nhớ trạng thái giữa các batch liên tiếp), bạn BẮT BUỘC phải làm gì sau khi kết thúc mỗi epoch?","[""A. model.save()"", ""B. model.reset_states() (để xóa trạng thái ẩn trước khi bắt đầu epoch mới trên cùng dữ liệu)."", ""C. model.clear_session()"", ""D. Tăng learning rate.""]",B,"[""15.5_ThucHanh_RNN""]",hard
Chg15_Pra_Q6,"Để xây dựng kiến trúc WaveNet (dùng CNN để xử lý chuỗi) trong Keras, ta sử dụng lớp `Conv1D` với tham số quan trọng nào để mở rộng vùng nhìn (receptive field) mà không cần tăng quá nhiều tham số?","[""A. strides=2"", ""B. padding='valid'"", ""C. dilation_rate (ví dụ: 1, 2, 4, 8...)"", ""D. kernel_size=100""]",C,"[""15.5_ThucHanh_RNN""]",medium
Chg15_Pra_Q7,"Lớp `Bidirectional(LSTM(10))` có tác dụng gì?","[""A. Tạo ra 2 lớp LSTM chồng lên nhau."", ""B. Tạo ra 2 lớp LSTM song song: một lớp đọc chuỗi từ trái sang phải, một lớp đọc từ phải sang trái, sau đó nối (concat) kết quả của chúng lại."", ""C. Tăng gấp đôi tốc độ huấn luyện."", ""D. Giảm một nửa số tham số.""]",B,"[""15.5_ThucHanh_RNN""]",easy
Chg15_Pra_Q8,"Để xử lý các chuỗi có độ dài khác nhau (variable length) được đệm bằng số 0 (padding), bạn nên thêm lớp nào vào đầu mô hình để các lớp sau bỏ qua các giá trị 0 đó?","[""A. Masking(mask_value=0.0)"", ""B. Embedding()"", ""C. Padding()"", ""D. Reshape()""]",A,"[""15.5_ThucHanh_RNN""]",easy
Chg15_Pra_Q9,"Hàm mất mát `Huber` thường được ưa chuộng hơn MSE trong dự báo chuỗi thời gian khi dữ liệu có đặc điểm gì?","[""A. Dữ liệu rất sạch."", ""B. Dữ liệu có nhiều điểm ngoại lai (outliers) nhiễu, vì Huber loss ít nhạy cảm với sai số lớn hơn so với MSE (nó kết hợp tính chất của MAE và MSE)."", ""C. Dữ liệu là nhị phân."", ""D. Dữ liệu quá ngắn.""]",B,"[""15.5_ThucHanh_RNN""]",medium
Chg15_Pra_Q10,"Nếu bạn muốn tự viết một Cell RNN tùy chỉnh (Custom RNN Cell) để dùng trong lớp `RNN(MyCell())`, thuộc tính `state_size` trong class của bạn quy định điều gì?","[""A. Kích thước đầu vào."", ""B. Số lượng unit (kích thước vector trạng thái ẩn) của cell đó."", ""C. Số lượng epoch."", ""D. Kích thước batch.""]",B,"[""15.5_ThucHanh_RNN""]",hard

Chg16_Q1_MCQ,"Word Embeddings (như Word2Vec) ưu việt hơn One-Hot Encoding ở điểm nào?","[""A. Dễ cài đặt hơn."", ""B. Tạo ra các vector dày đặc (Dense) có số chiều thấp hơn và đặc biệt là nắm bắt được 'Ý nghĩa ngữ nghĩa' (các từ đồng nghĩa sẽ nằm gần nhau trong không gian vector)."", ""C. Không cần huấn luyện."", ""D. Xử lý được từ mới chưa gặp bao giờ.""]",B,"[""16.1_Word_Embeddings""]",medium
Chg16_Q2_MCQ,"Cơ chế 'Attention' (Chú ý) giúp mô hình Encoder-Decoder cải thiện dịch máy như thế nào?","[""A. Nó giúp mô hình chạy nhanh hơn."", ""B. Nó cho phép Decoder 'nhìn' vào toàn bộ câu gốc và tập trung (focus) vào các từ khóa liên quan nhất tại mỗi bước dịch, thay vì chỉ dựa vào một vector ngữ cảnh nén duy nhất."", ""C. Nó tự động sửa lỗi chính tả."", ""D. Nó loại bỏ các từ vô nghĩa.""]",B,"[""16.3_Attention_Is_All_You_Need""]",hard
Chg16_Q3_MCQ,"Tại sao kiến trúc Transformer lại thay thế RNN trong hầu hết các tác vụ NLP hiện đại?","[""A. Vì nó nhỏ gọn hơn."", ""B. Vì nó cho phép 'Song song hóa' (Parallelization) quá trình huấn luyện (không cần chờ từ trước xong mới đến từ sau như RNN) và xử lý tốt các phụ thuộc xa nhờ cơ chế Self-Attention."", ""C. Vì Google tạo ra nó."", ""D. Vì nó không cần GPU.""]",B,"[""16.3_Attention_Is_All_You_Need""]",hard
Chg16_Q4_MCQ,"Sự khác biệt cơ bản về kiến trúc giữa BERT và GPT là gì?","[""A. BERT là mô hình Encoder (hai chiều, giỏi hiểu ngữ cảnh). GPT là mô hình Decoder (tự hồi quy, giỏi sinh văn bản tiếp theo)."", ""B. BERT dùng để sinh văn bản, GPT dùng để phân loại."", ""C. BERT của OpenAI, GPT của Google."", ""D. Không có sự khác biệt.""]",A,"[""16.4_BERT_GPT_HuggingFace""]",hard

Chg16_Pra_Q3,"Để xây dựng một mô hình Char-RNN (Dự đoán ký tự tiếp theo) bằng Keras, khi khởi tạo `Tokenizer`, bạn cần thiết lập tham số nào để nó phân tách văn bản theo từng ký tự thay vì từng từ?","[""A. char_level=True"", ""B. split_char=True"", ""C. word_level=False"", ""D. token_mode='char'""]",A,"[""16.5_ThucHanh_NLP""]",easy
Chg16_Pra_Q4,"Khi xử lý các câu có độ dài khác nhau bằng cách đệm (padding), để lớp `LSTM` hoặc `GRU` tự động bỏ qua các token đệm (padding tokens - thường là số 0), bạn cần cấu hình gì ở lớp `Embedding` đầu tiên?","[""A. trainable=False"", ""B. mask_zero=True"", ""C. padding='valid'"", ""D. ignore_zeros=True""]",B,"[""16.5_ThucHanh_NLP""]",medium
Chg16_Pra_Q5,"Khi xây dựng một 'Stateful RNN' (RNN có nhớ trạng thái giữa các batch), điều kiện bắt buộc về kích thước batch (batch size) khi khai báo lớp RNN là gì?","[""A. Batch size phải bằng 1."", ""B. Phải xác định rõ `batch_input_shape` ngay từ đầu và giữ kích thước batch cố định trong suốt quá trình huấn luyện."", ""C. Batch size phải là số chẵn."", ""D. Không có điều kiện gì.""]",B,"[""16.5_ThucHanh_NLP""]",hard
Chg16_Pra_Q6,"Trong Keras, để biến một lớp `LSTM` thường thành `Bidirectional LSTM` (LSTM hai chiều), ta dùng cú pháp nào?","[""A. Bidirectional(LSTM(32))"", ""B. LSTM(32, direction='bi')"", ""C. LSTM(32, bidirectional=True)"", ""D. BiLSTM(32)""]",A,"[""16.5_ThucHanh_NLP""]",easy
Chg16_Pra_Q7,"Mô hình Transformer không sử dụng RNN hay CNN, vậy nó làm thế nào để nhận biết thứ tự của các từ trong câu (ví dụ: 'tôi đá bóng' khác 'bóng đá tôi')?","[""A. Nó không cần biết thứ tự."", ""B. Nó cộng thêm vector 'Positional Encoding' (Mã hóa vị trí - sử dụng hàm sin/cos) vào vector nhúng (embedding) của từ trước khi đưa vào mạng."", ""C. Nó sử dụng một biến đếm chạy."", ""D. Nó thêm chỉ số index vào cuối mỗi từ.""]",B,"[""16.5_ThucHanh_NLP""]",medium
Chg16_Pra_Q8,"Lớp `MultiHeadAttention` trong Keras có tham số `num_heads`. Tham số này đại diện cho điều gì?","[""A. Số lượng lớp ẩn."", ""B. Số lượng không gian biểu diễn (representation subspaces) độc lập mà mô hình sẽ học song song (ví dụ: một head tập trung vào quan hệ chủ-vị, head khác tập trung vào quan hệ thì-thời gian)."", ""C. Số lượng từ trong câu."", ""D. Kích thước của vector đầu ra.""]",B,"[""16.5_ThucHanh_NLP""]",medium
Chg16_Pra_Q9,"Khi xây dựng mô hình Dịch máy (NMT) dạng Encoder-Decoder cơ bản với RNN, làm thế nào để truyền 'ngữ cảnh' (context) từ Encoder sang Decoder?","[""A. Nối đầu ra của Encoder vào đầu vào của Decoder."", ""B. Lấy trạng thái ẩn cuối cùng (final states) của Encoder và dùng nó làm trạng thái khởi tạo (initial_state) cho lớp RNN của Decoder."", ""C. Dùng cơ chế Attention."", ""D. Copy trọng số của Encoder sang Decoder.""]",B,"[""16.5_ThucHanh_NLP""]",hard
Chg16_Pra_Q10,"Thư viện `tensorflow_text` (hoặc Tokenizer của Hugging Face) thường sử dụng kỹ thuật 'Subword Tokenization' (ví dụ: Byte Pair Encoding - BPE) để giải quyết vấn đề gì?","[""A. Giảm kích thước từ điển (Vocabulary size) và xử lý được các từ chưa từng gặp (OOV) bằng cách ghép chúng từ các mảnh từ nhỏ hơn (ví dụ: 'unhappiness' = 'un' + 'happi' + 'ness')."", ""B. Tăng tốc độ huấn luyện."", ""C. Loại bỏ dấu câu."", ""D. Chuyển văn bản thành chữ thường.""]",A,"[""16.5_ThucHanh_NLP""]",medium

Chg17_Q1_MCQ,"Mục tiêu chính của Autoencoder là gì?","[""A. Phân loại ảnh."", ""B. Học cách sao chép đầu vào ra đầu ra thông qua một 'nút thắt cổ chai' (bottleneck), qua đó ép mạng phải học một biểu diễn nén (compressed representation) của dữ liệu."", ""C. Tạo ra dữ liệu mới ngẫu nhiên."", ""D. Tăng kích thước dữ liệu.""]",B,"[""17.1_Autoencoders_BieuDien""]",medium
Chg17_Q2_MCQ,"Sự khác biệt cốt lõi giữa Variational Autoencoder (VAE) và Autoencoder thường là gì?","[""A. VAE không gian ẩn (latent space) là xác suất (Gaussian distribution), cho phép ta lấy mẫu (sample) để sinh ra dữ liệu mới chưa từng thấy."", ""B. VAE chạy nhanh hơn."", ""C. VAE dùng cho văn bản, Autoencoder dùng cho ảnh."", ""D. VAE không có nút thắt cổ chai.""]",A,"[""17.2_Variational_Autoencoders""]",hard
Chg17_Q3_MCQ,"Trong kiến trúc GAN (Generative Adversarial Networks), vai trò của Generator và Discriminator là gì?","[""A. Hợp tác với nhau."", ""B. Đối nghịch nhau: Generator cố gắng tạo ra ảnh giả để lừa Discriminator, còn Discriminator cố gắng phân biệt đâu là ảnh thật, đâu là ảnh giả."", ""C. Generator tạo dữ liệu, Discriminator xóa dữ liệu."", ""D. Cả hai đều cố gắng phân loại ảnh.""]",B,"[""17.3_GANs_Adversarial""]",easy
Chg17_Q4_MCQ,"Diffusion Models (như Stable Diffusion, DALL-E) hoạt động dựa trên nguyên lý nào?","[""A. Ghép các ảnh lại với nhau."", ""B. Quá trình phá hủy và khôi phục: Thêm nhiễu (noise) dần dần vào ảnh cho đến khi thành nhiễu trắng, rồi học cách đảo ngược quá trình đó để khôi phục ảnh rõ nét từ nhiễu."", ""C. Sử dụng cây quyết định."", ""D. Chỉ dùng GANs.""]",B,"[""17.4_Diffusion_Models""]",hard

Chg17_Pra_Q3,"Khi xây dựng một 'Stacked Autoencoder' để nén ảnh (ví dụ Fashion MNIST, pixel 0-1), nếu lớp đầu ra sử dụng hàm kích hoạt `sigmoid`, hàm mất mát (loss function) nào là phù hợp nhất để tối ưu hóa việc tái tạo ảnh?","[""A. 'mean_squared_error'"", ""B. 'binary_crossentropy' (xem mỗi pixel như một xác suất Bernoulli)."", ""C. 'categorical_crossentropy'"", ""D. 'hinge' ""]",B,"[""17.5_ThucHanh_GenAI""]",medium
Chg17_Pra_Q4,"Trong kiến trúc 'Denoising Autoencoder' (Autoencoder khử nhiễu), khi gọi hàm `model.fit()`, dữ liệu đầu vào và nhãn mục tiêu (target) phải như thế nào?","[""A. model.fit(X_train, X_train)"", ""B. model.fit(X_train_noisy, X_train_noisy)"", ""C. model.fit(X_train_noisy, X_train) (Đầu vào là ảnh nhiễu, mục tiêu là ảnh gốc sạch)."", ""D. model.fit(X_train, y_train)""]",C,"[""17.5_ThucHanh_GenAI""]",easy
Chg17_Pra_Q5,"Để cài đặt lớp lấy mẫu (Sampling Layer) trong Variational Autoencoder (VAE) sử dụng 'Reparameterization Trick', công thức nào sau đây là đúng để tính vector ẩn `z` từ `z_mean` và `z_log_var`?","[""A. z = z_mean + z_log_var"", ""B. z = z_mean + exp(0.5 * z_log_var) * epsilon (với epsilon là nhiễu ngẫu nhiên chuẩn)."", ""C. z = z_mean * z_log_var"", ""D. z = random.normal(z_mean, z_log_var)""]",B,"[""17.5_ThucHanh_GenAI""]",hard
Chg17_Pra_Q6,"Trong vòng lặp huấn luyện tùy chỉnh cho GAN, tại bước huấn luyện Generator (thông qua mô hình gan gồm generator nối vào discriminator), tại sao ta phải đặt `discriminator.trainable = False`?","[""A. Để tiết kiệm bộ nhớ GPU."", ""B. Để tránh việc Discriminator học cách chấp nhận ảnh giả trong khi ta chỉ muốn cập nhật trọng số của Generator để nó tạo ảnh tốt hơn (lừa được Discriminator hiện tại)."", ""C. Để Generator không học được gì."", ""D. Scikit-Learn yêu cầu vậy.""]",B,"[""17.5_ThucHanh_GenAI""]",hard
Chg17_Pra_Q7,"Khi xây dựng Generator cho mô hình DCGAN (Deep Convolutional GAN) để sinh ảnh, lớp nào thường được sử dụng để tăng kích thước không gian (Upsampling) từ vector nhiễu lên thành ảnh?","[""A. Conv2D(strides=2)"", ""B. MaxPooling2D()"", ""C. Conv2DTranspose() (Chuyển vị tích chập)."", ""D. Flatten()""]",C,"[""17.5_ThucHanh_GenAI""]",medium
Chg17_Pra_Q8,"Để buộc Autoencoder học các đặc trưng thưa (Sparse Autoencoder), ta thường thêm regularization (như L1) vào đâu?","[""A. Vào trọng số kernel (`kernel_regularizer`)."", ""B. Vào đầu ra của lớp mã hóa (`activity_regularizer`), để ép phần lớn các neuron trong lớp mã hóa này có giá trị gần bằng 0."", ""C. Vào hàm loss tái tạo."", ""D. Vào bias.""]",B,"[""17.5_ThucHanh_GenAI""]",hard
Chg17_Pra_Q9,"Hàm mất mát của Variational Autoencoder (VAE) bao gồm hai thành phần là gì?","[""A. Reconstruction Loss (Lỗi tái tạo) và Latent Loss (KL Divergence - đo độ lệch giữa phân phối của mã tiềm ẩn và phân phối chuẩn)."", ""B. Generator Loss và Discriminator Loss."", ""C. MSE và MAE."", ""D. Accuracy và Precision.""]",A,"[""17.5_ThucHanh_GenAI""]",medium
Chg17_Pra_Q10,"Hiện tượng 'Mode Collapse' trong huấn luyện GAN biểu hiện như thế nào trong thực tế?","[""A. Generator tạo ra nhiễu trắng."", ""B. Generator chỉ tạo ra một vài mẫu ảnh giống hệt nhau (hoặc rất ít đa dạng), bất kể đầu vào nhiễu là gì."", ""C. Discriminator luôn đoán sai."", ""D. Quá trình huấn luyện dừng lại ngay lập tức.""]",B,"[""17.5_ThucHanh_GenAI""]",medium

Chg18_Q1_MCQ,"Trong bài toán Học tăng cường, mục tiêu tối thượng của Tác tử (Agent) là gì?","[""A. Hoàn thành game nhanh nhất."", ""B. Tối đa hóa tổng phần thưởng tích lũy (Cumulative Reward) theo thời gian."", ""C. Tiêu diệt hết kẻ thù."", ""D. Không làm gì cả.""]",B,"[""18.1_Policy_Reward""]",easy
Chg18_Q2_MCQ,"Chiến lược 'Khám phá' (Exploration) và 'Khai thác' (Exploitation) khác nhau thế nào?","[""A. Khai thác là tìm vùng đất mới, Khám phá là ở lại vùng cũ."", ""B. Khám phá là thử các hành động mới để tìm hiểu môi trường (chấp nhận rủi ro). Khai thác là thực hiện hành động tốt nhất đã biết để nhận thưởng ngay."", ""C. Cả hai là một."", ""D. Khám phá chỉ dùng trong Supervised Learning.""]",B,"[""18.2_Q_Learning""]",medium
Chg18_Q3_MCQ,"Hệ số chiết khấu (Discount Factor - Gamma) trong phương trình Bellman dùng để làm gì?","[""A. Để giảm giá sản phẩm."", ""B. Để xác định mức độ quan trọng của phần thưởng trong tương lai so với phần thưởng hiện tại (Gamma gần 0: chỉ quan tâm hiện tại; Gamma gần 1: quan tâm lâu dài)."", ""C. Để tăng tốc độ học."", ""D. Để phạt agent.""]",B,"[""18.2_Q_Learning""]",medium
Chg18_Q4_MCQ,"Deep Q-Network (DQN) đã cải tiến Q-Learning cổ điển như thế nào để chơi được các game Atari phức tạp?","[""A. Sử dụng Mạng nơ-ron (CNN) để xấp xỉ hàm giá trị Q (Q-value function) thay vì dùng bảng tra cứu (Q-table) quá lớn."", ""B. Dùng chuột máy tính."", ""C. Dùng quy tắc if-else."", ""D. Loại bỏ phần thưởng.""]",A,"[""18.3_Deep_Q_Networks""]",hard

Chg18_Pra_Q3,"Trong thư viện Gym/Gymnasium, lệnh `env.action_space.sample()` dùng để làm gì?","[""A. Lấy mẫu một hành động ngẫu nhiên hợp lệ từ không gian hành động (dùng cho giai đoạn khám phá hoặc kiểm tra ngẫu nhiên)."", ""B. Lấy hành động tốt nhất hiện tại."", ""C. Lấy danh sách tất cả hành động."", ""D. Reset môi trường.""]",A,"[""18.6_ThucHanh_RL""]",easy
Chg18_Pra_Q4,"Khi đưa `observation` (quan sát) từ môi trường Gym vào mô hình Keras để dự đoán hành động, tại sao ta thường phải dùng lệnh `obs[np.newaxis]` (hoặc `tf.expand_dims`)?","[""A. Để chuẩn hóa dữ liệu."", ""B. Để thêm chiều batch (batch dimension) vào đầu, vì Keras luôn mong đợi đầu vào có dạng (batch_size, features), ngay cả khi batch_size=1."", ""C. Để chuyển sang kiểu float32."", ""D. Để xóa chiều thừa.""]",B,"[""18.6_ThucHanh_RL""]",medium
Chg18_Pra_Q5,"Trong thuật toán DQN, 'Replay Buffer' (Bộ nhớ phát lại) thường được cài đặt bằng cấu trúc dữ liệu nào trong Python để tự động xóa các mẫu cũ nhất khi bộ nhớ đầy?","[""A. `collections.deque(maxlen=2000)`"", ""B. `list()`"", ""C. `set()`"", ""D. `dict()`""]",A,"[""18.6_ThucHanh_RL""]",medium
Chg18_Pra_Q6,"Chiến lược 'Epsilon-Greedy' trong code thường được viết như thế nào?","[""A. Luôn chọn `np.argmax(model.predict(obs))`."", ""B. Sinh số ngẫu nhiên `r`. Nếu `r < epsilon`: chọn hành động ngẫu nhiên (khám phá). Ngược lại: chọn hành động có Q-value cao nhất (khai thác)."", ""C. Nếu `epsilon > 0.5` thì dừng lại."", ""D. Luôn chọn hành động ngẫu nhiên.""]",B,"[""18.6_ThucHanh_RL""]",medium
Chg18_Pra_Q7,"Khi huấn luyện DQN, tại sao ta cần cập nhật `target_model` (mạng mục tiêu) chậm hơn so với `model` chính (ví dụ: sao chép trọng số sau mỗi 50 bước)?","[""A. Để tiết kiệm pin."", ""B. Để ổn định quá trình huấn luyện. Nếu cập nhật liên tục, mục tiêu (target) sẽ di chuyển liên tục theo mô hình đang học, khiến mô hình giống như 'chú chó tự đuổi theo đuôi mình', gây dao động và khó hội tụ."", ""C. Để tăng tốc độ GPU."", ""D. Scikit-Learn yêu cầu vậy.""]",B,"[""18.6_ThucHanh_RL""]",hard
Chg18_Pra_Q8,"Hàm mất mát (Loss) tùy chỉnh cho thuật toán Policy Gradients (như REINFORCE) được tính như thế nào trong TensorFlow?","[""A. `loss = -tf.reduce_mean(log_prob * discounted_reward)` (Dấu trừ để biến bài toán tối đa hóa phần thưởng thành tối thiểu hóa loss)."", ""B. `loss = mse(y_true, y_pred)`"", ""C. `loss = accuracy`"", ""D. `loss = tf.reduce_sum(rewards)`""]",A,"[""18.6_ThucHanh_RL""]",hard
Chg18_Pra_Q9,"Khi sử dụng thư viện `TF-Agents`, để bọc (wrap) một môi trường Gym tiêu chuẩn thành môi trường TensorFlow (xử lý Tensors thay vì NumPy arrays), ta dùng hàm nào?","[""A. `tf_agents.environments.suite_gym.load(env_name)`"", ""B. `tf_agents.environments.TFPyEnvironment(env)`"", ""C. `tf.convert_to_tensor(env)`"", ""D. `gym.make(env_name)`""]",B,"[""18.6_ThucHanh_RL""]",hard
Chg18_Pra_Q10,"Để xử lý ảnh đầu vào từ game Atari (ví dụ 210x160 pixel màu) cho DQN, bước tiền xử lý nào là quan trọng nhất để giảm tải tính toán?","[""A. Tăng độ phân giải."", ""B. Cắt (Crop) vùng quan trọng, chuyển sang đen trắng (Grayscale) và giảm kích thước (Downsampling) xuống khoảng 84x84 pixel."", ""C. Xoay ảnh liên tục."", ""D. Giữ nguyên ảnh gốc.""]",B,"[""18.6_ThucHanh_RL""]",medium

Chg19_Q1_MCQ,"TensorFlow Serving được thiết kế để giải quyết vấn đề gì trong môi trường sản xuất (Production)?","[""A. Huấn luyện mô hình."", ""B. Phục vụ mô hình (Model Serving) hiệu năng cao qua API (gRPC/REST), hỗ trợ quản lý phiên bản mô hình và xử lý nhiều yêu cầu đồng thời."", ""C. Viết code Python."", ""D. Tạo giao diện web.""]",B,"[""19.1_TF_Serving_Server""]",medium
Chg19_Q2_MCQ,"Kỹ thuật 'Quantization' (Lượng tử hóa) trong TensorFlow Lite giúp ích gì khi đưa AI lên điện thoại?","[""A. Làm mô hình thông minh hơn."", ""B. Giảm độ chính xác của các con số (ví dụ: từ float32 xuống int8) để giảm mạnh kích thước mô hình và tăng tốc độ chạy mà ít làm giảm độ chính xác."", ""C. Tăng kích thước mô hình."", ""D. Chuyển mô hình lên đám mây.""]",B,"[""19.2_TF_Lite_Mobile""]",medium
Chg19_Q3_MCQ,"Tại sao việc giám sát (Monitoring) 'Data Drift' lại quan trọng sau khi triển khai mô hình?","[""A. Để xem ổ cứng có đầy không."", ""B. Vì dữ liệu thực tế thay đổi theo thời gian (ví dụ: thói quen người dùng thay đổi), làm giảm hiệu suất mô hình nếu không được phát hiện để huấn luyện lại."", ""C. Để tính lương cho kỹ sư."", ""D. Không quan trọng.""]",B,"[""19.4_GiamSat_BaoTri""]",medium

Chg19_Pra_Q3,"Khi xuất mô hình bằng `model.save('my_model', save_format='tf')`, thư mục kết quả chứa file `saved_model.pb` và hai thư mục con nào?","[""A. `weights` và `biases`."", ""B. `variables` (chứa giá trị trọng số) và `assets` (chứa các file phụ trợ như từ điển từ vựng)."", ""C. `json` và `bin`."", ""D. `train` và `test`.""]",B,"[""19.5_ThucHanh_Deploy""]",easy
Chg19_Pra_Q4,"Để kích hoạt 'Dynamic Range Quantization' (lượng tử hóa phạm vi động) khi chuyển đổi sang TFLite nhằm giảm kích thước mô hình 4 lần, bạn cần thiết lập thuộc tính nào của `converter`?","[""A. converter.optimizations = [tf.lite.Optimize.DEFAULT]"", ""B. converter.quantize_weights = True"", ""C. converter.target_spec.supported_types = [tf.float16]"", ""D. converter.shrink = True""]",A,"[""19.5_ThucHanh_Deploy""]",medium
Chg19_Pra_Q5,"Khi gửi yêu cầu dự đoán đến TensorFlow Serving qua giao thức REST API, dữ liệu JSON phải có cấu trúc như thế nào (giả sử mô hình nhận input chuẩn)?","[""A. `{'data': [1, 2, 3]}`"", ""B. `{'instances': [[1.0, 2.0, ...], [3.0, 4.0, ...]]}` (hoặc key 'inputs' cho cấu trúc columnar)."", ""C. `{'prediction': [1, 2, 3]}`"", ""D. `{'image': 'base64...'}`""]",B,"[""19.5_ThucHanh_Deploy""]",medium
Chg19_Pra_Q6,"Để huấn luyện mô hình Keras trên nhiều GPU (Multi-GPU) trên cùng một máy tính, bạn cần đặt code tạo và compile mô hình bên trong phạm vi (scope) của chiến lược nào?","[""A. `with tf.device('/CPU:0'):`"", ""B. `with tf.distribute.MirroredStrategy().scope():`"", ""C. `with tf.distribute.TPUStrategy().scope():`"", ""D. `with tf.Session():`""]",B,"[""19.5_ThucHanh_Deploy""]",medium
Chg19_Pra_Q7,"Công cụ dòng lệnh (CLI) nào được sử dụng để chuyển đổi mô hình `SavedModel` sang định dạng dùng cho trình duyệt web (TensorFlow.js)?","[""A. `tflite_convert`"", ""B. `tensorflowjs_converter`"", ""C. `tf_upgrade_v2`"", ""D. `keras_to_js`""]",B,"[""19.5_ThucHanh_Deploy""]",easy
Chg19_Pra_Q8,"Khi sử dụng `TF Serving`, để kiểm tra thông tin về phiên bản mô hình và trạng thái (metadata) qua REST API, bạn gửi request đến URL nào?","[""A. `/v1/models/my_model`"", ""B. `/v1/models/my_model/metadata`"", ""C. `/status`"", ""D. `/predict`""]",A,"[""19.5_ThucHanh_Deploy""]",hard
Chg19_Pra_Q9,"Lớp `tf.lookup.StaticVocabularyTable` thường được dùng trong Serving để làm gì?","[""A. Để vẽ biểu đồ."", ""B. Để thực hiện tra cứu (lookup) từ điển (ví dụ: chuyển từ thành ID) ngay bên trong đồ thị mô hình (in-graph preprocessing), giúp mô hình nhận input là văn bản thô thay vì số nguyên."", ""C. Để lưu trữ ảnh."", ""D. Để mã hóa One-hot.""]",B,"[""19.5_ThucHanh_Deploy""]",hard
Chg19_Pra_Q10,"Khi triển khai trên thiết bị di động, nếu TFLite Model yêu cầu các phép toán (Ops) mà TensorFlow Lite runtime tiêu chuẩn không hỗ trợ (ví dụ một số hàm loss lạ), giải pháp là gì?","[""A. Không thể chạy được."", ""B. Bật cờ `converter.allow_custom_ops = True` và sử dụng 'Select TF ops' (chấp nhận mô hình nặng hơn vì phải kèm theo thư viện TF gốc)."", ""C. Viết lại toàn bộ mô hình bằng C++."", ""D. Dùng server.""]",B,"[""19.5_ThucHanh_Deploy""]",hard